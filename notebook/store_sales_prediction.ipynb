{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"border-width:2px; border-style:solid; border-color:blue; background-color:#367395; text-align: center; text-shadow: 0px 0px 3px red; padding-bottom: 2em; padding-top: 1em;\">ROSSMANN REVENUE FORECASTING<br>Previsão de Faturamento Utilizando Regressão</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COURSE: \"DS IN PRODUCTION\" - Comunidade DS\n",
    "- Student: Manoel Luiz Menezes Mendonça\n",
    "- Instructor: Meigarom Lopes\n",
    "- Period: 2023/october to 2024/february\n",
    "- Our portfolio: http://menezes.mendonca.nom.br/\n",
    "- Our Linkedin: https://www.linkedin.com/in/manoelmendonca-eng-adv/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.0. INITIAL PROCEDURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable running slower parts when not needed... (EX: when re-running the code)\n",
    "\n",
    "BOL_RUN_BORUTA     = True     # see step-6 (feature selection), items 6.2 & 6.3\n",
    "BOL_RUN_MLMODELS   = True     # see step-7 (machine learning models)\n",
    "BOL_RUN_FINETUNING = True     # see step-8 (hyperparams fine tuning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import datetime\n",
    "import pandas      as pd\n",
    "import numpy       as np\n",
    "import inflection\n",
    "import random\n",
    "import pickle\n",
    "import warnings\n",
    "#import requests\n",
    "import re\n",
    "import json\n",
    "warnings.filterwarnings( 'ignore' )\n",
    "\n",
    "from scipy                 import stats as ss\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble      import RandomForestRegressor\n",
    "from sklearn.metrics       import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error\n",
    "from sklearn.linear_model  import LinearRegression, Lasso\n",
    "from tabulate              import tabulate\n",
    "from boruta                import BorutaPy\n",
    "from flask                 import Flask, request, Response\n",
    "\n",
    "from matplotlib            import pyplot as plt\n",
    "\n",
    "#import seaborn    as sns\n",
    "#import xgboost    as xgb\n",
    "\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "from IPython.display      import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2. Various Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#............... In case of categorical attributes: Cramer V Method\n",
    "# https://en.wikipedia.org/wiki/Cram%C3%A9r%27s_V\n",
    "\n",
    "def cramer_v( x, y ):\n",
    "    confusion_matrix = pd.crosstab( x, y ).to_numpy()\n",
    "    chi2 = ss.chi2_contingency( confusion_matrix )[0]\n",
    "    n = confusion_matrix.sum()\n",
    "    r, k = confusion_matrix.shape\n",
    "\n",
    "    chi2corr = max( 0, chi2 - (k-1)*(r-1)/(n-1) )\n",
    "\n",
    "    kcorr = k - (k-1)**2/(n-1)\n",
    "    rcorr = r - (r-1)**2/(n-1)\n",
    "\n",
    "    return np.sqrt( (chi2corr/n) / ( min(kcorr-1, rcorr-1) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#............... FUNCTION: ml_error\n",
    "# Step 7\n",
    "def ml_error( model_name, y, yhat ):\n",
    "    mae = mean_absolute_error( y, yhat )\n",
    "    mape = mean_absolute_percentage_error( y, yhat )\n",
    "    rmse = np.sqrt( mean_squared_error( y, yhat ) )\n",
    "\n",
    "    return pd.DataFrame( { 'Model Name': model_name,\n",
    "                           'MAE': mae,\n",
    "                           'MAPE': mape,\n",
    "                           'RMSE': rmse }, index=[0] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#............... FUNCTION: CROSS_VALIDATION\n",
    "# Module 7, Class 33\n",
    "\n",
    "def cross_validation( x_training, kfold, model_name, model, verbose=False ):\n",
    "\n",
    "    mae_list = []\n",
    "    mape_list = []\n",
    "    rmse_list = []\n",
    "\n",
    "    for k in reversed( range( 1, kfold+1 ) ):\n",
    "        if verbose:\n",
    "            print( 'KFold Number: {}'.format(k) )\n",
    "        # start and end date for validation\n",
    "        validation_start_date = x_training['date'].max() - datetime.timedelta( days=k*6*7 )\n",
    "        validation_end_date = x_training['date'].max() - datetime.timedelta( days=(k-1)*6*7 )\n",
    "        if verbose:\n",
    "            print( 'Dates -     start={}     end={}'.format( validation_start_date, validation_end_date ) )\n",
    "\n",
    "        # filtering dataset\n",
    "        training = x_training[x_training['date'] < validation_start_date]\n",
    "        validation = x_training[(x_training['date'] >= validation_start_date) & (x_training['date'] <= validation_end_date)]\n",
    "\n",
    "        # training and validation dataset (exclude two features)\n",
    "        xtraining = training.drop( ['date', 'sales'], axis=1 )\n",
    "        ytraining = training['sales']\n",
    "\n",
    "        xvalidation = validation.drop( ['date', 'sales'], axis=1 )\n",
    "        yvalidation = validation['sales']\n",
    "\n",
    "        # model\n",
    "        m = model.fit( xtraining, ytraining )\n",
    "\n",
    "        # prediction\n",
    "        yhat = m.predict(xvalidation)\n",
    "\n",
    "        # performance\n",
    "        m_result = ml_error( model_name, np.expm1(yvalidation), np.expm1(yhat) )\n",
    "        if verbose:\n",
    "            print( m_result )\n",
    "\n",
    "        # store performance of each KFold iteration\n",
    "        mae_list.append(  m_result['MAE'] )\n",
    "        mape_list.append( m_result['MAPE'] )\n",
    "        rmse_list.append( m_result['RMSE'] )\n",
    "\n",
    "    return pd.DataFrame(\n",
    "            {'Model Name': model_name,\n",
    "             'MAE CV': np.round( np.mean(mae_list), 2 ).astype(str) + ' +/- ' + np.round( np.std(mae_list), 2 ).astype(str),\n",
    "             'MAPE CV': np.round( np.mean(mape_list), 2 ).astype(str) + ' +/- ' + np.round( np.std(mape_list), 2 ).astype(str),\n",
    "             'RMSE CV': np.round( np.mean(rmse_list), 2 ).astype(str) + ' +/- ' + np.round( np.std(rmse_list), 2 ).astype(str),\n",
    "             'rmse_float': np.mean(rmse_list) },\n",
    "             index=[0] )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.3. Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.2. LOADING DATA\n",
    "\n",
    "# my cloudio\n",
    "MyCSV = 'http://menezes.mendonca.nom.br/datasets/rossmann/train.csv'\n",
    "# local file\n",
    "#MyCSV = '../data/train.csv'\n",
    "df_sales_raw = pd.read_csv( MyCSV, low_memory=False )\n",
    "\n",
    "MyCSV = 'http://menezes.mendonca.nom.br/datasets/rossmann/store.csv'\n",
    "#MyCSV = '../data/store.csv'\n",
    "df_store_raw = pd.read_csv( MyCSV, low_memory=False )\n",
    "\n",
    "# merge\n",
    "df_raw = pd.merge( df_sales_raw, df_store_raw, how='left', on='Store' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 1. DATA DESCRIPTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Initial Procedure (copy data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.0. COPY DATA\n",
    "# Copy dataset to a new variable\n",
    "df1 = df_raw.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Rename Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1. RENAME COLUMNS\n",
    "cols_old = ['Store', 'DayOfWeek', 'Date', 'Sales', 'Customers', 'Open', 'Promo',\n",
    "            'StateHoliday', 'SchoolHoliday', 'StoreType', 'Assortment',\n",
    "            'CompetitionDistance', 'CompetitionOpenSinceMonth',\n",
    "            'CompetitionOpenSinceYear', 'Promo2', 'Promo2SinceWeek',\n",
    "            'Promo2SinceYear', 'PromoInterval' ]\n",
    "\n",
    "snakecase = lambda x: inflection.underscore( x )\n",
    "cols_new = list( map( snakecase, cols_old ) )\n",
    "\n",
    "# rename\n",
    "df1.columns = cols_new\n",
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.sample(8).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Data Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2. DATA DIMENSION\n",
    "print( 'Number of Rows: {}'.format( df1.shape[0] ) )\n",
    "print( 'Number of Cols: {}'.format( df1.shape[1] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.4. DATA TYPES\n",
    "# Convert OBJECT type to DATETIME64 type\n",
    "df1['date'] = pd.to_datetime( df1['date'] )\n",
    "df1.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df1.loc[0, 'date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. Check NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.4. CHECK NA\n",
    "df1.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6. Fillout NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.6. FILLOUT NA\n",
    "\n",
    "# ATTRIBUTE: competition_distance\n",
    "# criterion: if NA, then fill with a far away distance (very distant = 200.000)\n",
    "df1['competition_distance'] = (df1['competition_distance']\n",
    "        .apply( lambda x: 200000.0 if math.isnan( x ) else x ))\n",
    "\n",
    "# ATTRIBUTE: competition_open_since_month\n",
    "# criterion-1: if NA, then fill with the month/year of the current date\n",
    "# criterion-2: (to be tested) fill NA with first date (2013/01/01) or last date (2015/07/31)\n",
    "df1['competition_open_since_month'] = (df1\n",
    "        .apply( lambda x: x['date'].month if math.isnan( x['competition_open_since_month'] ) \n",
    "                                        else x['competition_open_since_month'], axis=1 ))\n",
    "\n",
    "# ATTRIBUTE: competition_open_since_year\n",
    "# criterion-1: if NA, then fill with the month/year of the current date\n",
    "# criterion-2: (to be tested) fill NA with first date (2013/01/01) or last date (2015/07/31)\n",
    "df1['competition_open_since_year'] = (df1\n",
    "        .apply( lambda x: x['date'].year if math.isnan( x['competition_open_since_year'] ) \n",
    "                                       else x['competition_open_since_year'], axis=1 ))\n",
    "\n",
    "# ATTRIBUTE: promo2_since_week\n",
    "# criterion-1: if NA, then fill with week of the current date\n",
    "# PS: ok, but attribute promo2 is not day-dependent, but store-dependent.\n",
    "# criterion-2: (to be tested) fill the NA gaps by running a Random Forest specific for this task.\n",
    "df1['promo2_since_week'] = (df1\n",
    "        .apply( lambda x: x['date'].week if math.isnan( x['promo2_since_week'] ) \n",
    "                                       else x['promo2_since_week'], axis=1 ))\n",
    "\n",
    "# ATTRIBUTE: promo2_since_year\n",
    "# criterion-1: if NA, then fill with the current year\n",
    "# PS: ok, but attribute promo2 is not day-dependent, but store-dependent.\n",
    "# criterion-2: (to be tested) fill the NA gaps by running a Random Forest specific for this task.\n",
    "df1['promo2_since_year'] = (df1\n",
    "        .apply( lambda x: x['date'].year if math.isnan( x['promo2_since_year'] ) \n",
    "                                       else x['promo2_since_year'], axis=1 ))\n",
    "\n",
    "# ATTRIBUTE: promo_interval\n",
    "df1['promo_interval'].fillna( 0, inplace=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.sample(6).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7. Change Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PS: 'date' attribute changed type in 1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.7. CHANGE TYPES\n",
    "# From FLOAT to INT\n",
    "df1['competition_open_since_month'] = df1['competition_open_since_month'].astype( np.int64 )\n",
    "df1['competition_open_since_year'] = df1['competition_open_since_year'].astype( np.int64 )\n",
    "df1['promo2_since_week']          = df1['promo2_since_week'].astype( np.int64 )\n",
    "df1['promo2_since_year']         = df1['promo2_since_year'].astype( np.int64 )\n",
    "df1.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.8. Descriptive Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8.1 Separete Numerical from Categorical Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.8. DESCRIPTIVE STATISTICS\n",
    "\n",
    "# Manually Separate: numerical features\n",
    "num_attr = [ 'sales', 'customers', 'competition_distance' ]\n",
    "#num_attributes = df1.select_dtypes( include=[ 'int64', 'float64' ] )\n",
    "num_attributes = df1.loc[:, num_attr].copy()\n",
    "\n",
    "# Categorical features: all the others\n",
    "cat_attr = []\n",
    "for a in df1.columns:\n",
    "    if a in num_attr:\n",
    "        #print(a)\n",
    "        continue\n",
    "    cat_attr.append(a)\n",
    "cat_attributes = df1.loc[:, cat_attr].copy()\n",
    "\n",
    "num_attributes.sample(8).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_attributes.sample(5).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8.2 Numerical Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.8.1 DESCRIPTIVE STATISTICS - NUMERICAL FEATURES\n",
    "# Dispersion (variance, standard deviation, max, min, range, skew, kurtosis)\n",
    "# central tendency (mean, median)\n",
    "\n",
    "# Central Tendency - mean, median\n",
    "ct1 = pd.DataFrame( num_attributes.apply( np.mean ) ).T\n",
    "ct2 = pd.DataFrame( num_attributes.apply( np.median ) ).T\n",
    "\n",
    "# Dispersion - std, min, max, range, skew, kurtosis\n",
    "d1 = pd.DataFrame( num_attributes.apply( np.std ) ).T\n",
    "d2 = pd.DataFrame( num_attributes.apply( min ) ).T\n",
    "d3 = pd.DataFrame( num_attributes.apply( max ) ).T\n",
    "d4 = pd.DataFrame( num_attributes.apply( lambda x: x.max() - x.min() ) ).T\n",
    "d5 = pd.DataFrame( num_attributes.apply( lambda x: x.skew() ) ).T\n",
    "d6 = pd.DataFrame( num_attributes.apply( lambda x: x.kurtosis() ) ).T\n",
    "\n",
    "# Concatenate the metrics\n",
    "m = pd.concat( [d2, d3, d4, ct1, ct2, d1, d5, d6] ).round(2).T.reset_index()\n",
    "m.columns = ['attributes', 'min', 'max', 'range', 'mean', 'median', 'std', 'skew', 'kurtosis' ]\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8.3 Categorical Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List of attributes & number of categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_attributes.apply( lambda x: x.unique().shape[0] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical attribute: (I) days of week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux1 = df1[ df1['sales'] > 0  ]\n",
    "plt.figure( figsize=(10, 5) )\n",
    "sns.boxplot( x='day_of_week', y='sales', data=aux1 );\n",
    "\n",
    "# table of quantities:\n",
    "total = aux1.shape[0]\n",
    "# In average, how many stores are open per weekday?\n",
    "aux11 = aux1.loc[:, ['day_of_week', 'store', 'store_type']].copy()\n",
    "aux12 = aux11.groupby(['day_of_week', 'store']).count()\n",
    "StoresPerDay = aux12.groupby('day_of_week').count().reset_index()\n",
    "StoresPerDay.columns = [ 'day_of_week', 'num_open_stores' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StoresPerDay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical attributes: (II) state holiday & school holiday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplot\n",
    "aux1 = df1[ df1['sales'] > 0  ]\n",
    "plt.figure( figsize=(10, 5) )\n",
    "plt.subplot( 1, 2, 1)\n",
    "sns.boxplot( x='state_holiday', y='sales', data=aux1 );\n",
    "plt.subplot( 1, 2, 2)\n",
    "sns.boxplot( x='school_holiday', y='sales', data=aux1 );\n",
    "\n",
    "# table of quantities:\n",
    "total = aux1.shape[0]\n",
    "aux11 = aux1.loc[:, ['state_holiday', 'store']].copy()\n",
    "stt_hol = aux11.groupby('state_holiday').count().reset_index()\n",
    "stt_hol['perc'] = ( 100. * stt_hol['store'] / total ).round(2)\n",
    "stt_hol.columns = [ 'state_holiday', 'qty', 'qty%' ]\n",
    "\n",
    "aux11 = aux1.loc[:, ['school_holiday', 'store']].copy()\n",
    "sch_hol = aux11.groupby('school_holiday').count().reset_index()\n",
    "sch_hol['perc'] = ( 100. * sch_hol['store'] / total ).round(2)\n",
    "sch_hol.columns = [ 'school_holiday', 'qty', 'qty%' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stt_hol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sch_hol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical attributes: (III) store_type & assortment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplot\n",
    "aux1 = df1[ df1['sales'] > 0  ]\n",
    "plt.figure( figsize=(10, 5) )\n",
    "plt.subplot( 1, 2, 1)\n",
    "sns.boxplot( x='store_type', y='sales', data=aux1, order='abcd' );\n",
    "plt.subplot( 1, 2, 2)\n",
    "sns.boxplot( x='assortment', y='sales', data=aux1, order='abc' );\n",
    "\n",
    "# tables of quantities:\n",
    "total = aux1.shape[0]\n",
    "aux11 = aux1.loc[:, ['store_type', 'store']].copy()\n",
    "st_type = aux11.groupby('store_type').count().reset_index()\n",
    "st_type['perc'] = ( 100. * st_type['store'] / total ).round(2)\n",
    "st_type.columns = [ 'store_type', 'qty', 'qty%' ]\n",
    "\n",
    "aux11 = aux1.loc[:, ['assortment', 'store']].copy()\n",
    "assortm = aux11.groupby('assortment').count().reset_index()\n",
    "assortm['perc'] = ( 100. * assortm['store'] / total ).round(2)\n",
    "assortm.columns = [ 'assortment', 'qty', 'qty%' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assortm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical attributes: (IV) promo, promo2, promo_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplots\n",
    "aux1 = df1[ df1['sales'] > 0  ]\n",
    "plt.figure( figsize=(10, 5) )\n",
    "plt.subplot( 1, 3, 1)\n",
    "sns.boxplot( x='promo', y='sales', data=aux1 );\n",
    "plt.yticks( fontsize=8 );\n",
    "plt.subplot( 1, 3, 2)\n",
    "sns.boxplot( x='promo2', y='sales', data=aux1 );\n",
    "plt.yticks( fontsize=8 );\n",
    "plt.subplot( 1, 3, 3)\n",
    "sns.boxplot( x='promo_interval', y='sales', data=aux1 );\n",
    "plt.xticks( rotation=15, fontsize=8 );\n",
    "plt.yticks( fontsize=8 );\n",
    "\n",
    "# table of quantities:\n",
    "total = aux1.shape[0]\n",
    "aux11 = aux1.loc[:, ['promo', 'store']].copy()\n",
    "prm = aux11.groupby('promo').count().reset_index()\n",
    "prm['perc'] = ( 100. * prm['store'] / total ).round(2)\n",
    "prm.columns = [ 'promo', 'qty', 'qty%' ]\n",
    "\n",
    "aux11 = aux1.loc[:, ['promo2', 'store']].copy()\n",
    "prm2 = aux11.groupby('promo2').count().reset_index()\n",
    "prm2['perc'] = ( 100. * prm2['store'] / total ).round(2)\n",
    "prm2.columns = [ 'promo2', 'qty', 'qty%' ]\n",
    "\n",
    "aux11 = aux1.loc[:, ['promo_interval', 'store']].copy()\n",
    "prmint = aux11.groupby('promo_interval').count().reset_index()\n",
    "prmint['perc'] = ( 100. * prmint['store'] / total ).round(2)\n",
    "prmint.columns = [ 'promo_interval', 'qty', 'qty%' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prm2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prmint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 2. FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0. Initial Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Hypotesis Mind Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('../img/MindMapHypothesisEN.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Hypotesis Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1. Hypothesis regarding STORES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.** Stores with larger staff should sell more\n",
    "\n",
    "**2.** Stores with higher stock should sell more\n",
    "\n",
    "**3.** Bigger stores should sell more\n",
    "\n",
    "**4.** Stores with higher assortment should sell more\n",
    "\n",
    "**5.** Stores close to other competitors should sell less\n",
    "\n",
    "**6.** Stores with long time competitors should sell more\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2. Hypothesis regarding PRODUCTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.** The more stores invest in marketing, the more they should sell\n",
    "\n",
    "**2.** The more stores expose their products, the more they should sell\n",
    "\n",
    "**3.** Stores with low budget products (like \"extra\" assortment) sell less than stores with more expensive products (like \"extended\" assortment).\n",
    "\n",
    "**4.** Stores with low budget products have more customers and stores with more expensive products have less customers.\n",
    "\n",
    "**5.** Most of the stores work with low budget products (like \"extra\" assortment).\n",
    "\n",
    "**6.** Stores with longer periods of promotional sales should sell more\n",
    "\n",
    "**7.** The more agressive the promotions, the more stores should sell\n",
    "\n",
    "**8.** Stores with more days of promotional sales should sell more\n",
    "\n",
    "**9.** Stores with more consecutive promotional sales should sell more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3. Hypothesis regarding TIME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.** Stores should sell more on Mondays, since most of the stores were closed the day before.\n",
    "\n",
    "**2.** On Sunday, the few stores open (33/1115) should sell more.\n",
    "\n",
    "**3.** On Saturday, the stores should sell less, due to the reduced working hours.\n",
    "\n",
    "**4.** Due to Mother's Day, May should be the second best selling month of the year.\n",
    "\n",
    "**5.** Due to the winter, January and February should sell less.\n",
    "\n",
    "**6.** Stores should sell more from the last days of the previous month to the first days of the current month, due to the receipt of salaries in the economy.\n",
    "\n",
    "**7.** Stores should sell less in the middle days of the month.\n",
    "\n",
    "**8.** Stores open during Christmas Holiday should sell more\n",
    "\n",
    "**9.** Stores should sell less during school holidays, since there is a reduced number of customers.\n",
    "\n",
    "**10.** Stores should also sell less during other holidays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Final List of Hypotesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main criteria: data is available (or not)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FINAL HYPOTHESIS LIST:\n",
    "\n",
    "**1.** Stores with long time competitors should sell more\n",
    "\n",
    "**2.** Stores close to other competitors should sell less\n",
    "\n",
    "**3.** Stores with low budget products (like \"extra\" assortment) sell less than stores with more expensive products (like \"extended\" assortment).\n",
    "\n",
    "**4.** Stores with low budget products have more customers and stores with more expensive products have less customers.\n",
    "\n",
    "**5.** Most of the stores work with low budget products (like \"extra\" assortment).\n",
    "\n",
    "**6.** Stores with longer periods of promotional sales should sell more\n",
    "\n",
    "**7.** Stores with more consecutive promotional sales should sell more\n",
    "\n",
    "**8.** Stores open during Christmas Holiday should sell more\n",
    "\n",
    "**9.** Stores should sell less during school holidays, since there is a reduced number of customers.\n",
    "\n",
    "**10.** Stores should also sell less during other holidays, since there is a reduced number of customers.\n",
    "\n",
    "**11.** Stores should sell more on Mondays, since most of the stores were closed the day before.\n",
    "\n",
    "**12.** On Sunday, the few stores open (33/1115) should sell more.\n",
    "\n",
    "**13.** On Saturday, the stores should sell less, due to the reduced working hours.\n",
    "\n",
    "**14.** Due to Mother's Day, May should be the second best selling month of the year.\n",
    "\n",
    "**15.** Due to the winter, January and February should sell less.\n",
    "\n",
    "**16.** Stores should sell more from the last days of the previous month to the first days of the current month, due to the receipt of salaries in the economy.\n",
    "\n",
    "**17.** Stores should sell less in the middle days of the month."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1 Attributes derived from 'PROMO2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attribute PROMO2: force to ZERO if date < (promo2_since_week + promo2_since_year)\n",
    "# PS: this is necessary because 'promo2' is originally a STORE.CSV attribute, not a TRAIN.CSV attribute\n",
    "# Example: STORE-28 started its 'promo2' only in 2015, 6th week. Before this week, promo2 should be ZERO.\n",
    "df2['promo2aux'] = df2.apply( lambda x: 0 if x['promo2'] == 0 \n",
    "                                          or x['date'].year < x['promo2_since_year'] \n",
    "                                          or (x['date'].year == x['promo2_since_year'] and x['date'].week < x['promo2_since_week']) \n",
    "                                          else 1, axis=1 )\n",
    "\n",
    "# Attribute \"IS_PROMO2\"\n",
    "# From \"promo2\" & \"promo_interval\", create \"is_promo2\" attribute\n",
    "\n",
    "month_map = { 1:'Jan', 2:'Feb', 3:'Mar',   4:'Apr',  5:'May',  6:'Jun', \n",
    "              7:'Jul', 8:'Aug', 9:'Sept', 10:'Oct', 11:'Nov', 12:'Dec' }\n",
    "\n",
    "df2['month_map'] = df2['date'].dt.month.map( month_map )\n",
    "\n",
    "df2['is_promo2'] = (df2[['promo_interval', 'promo2aux', 'month_map']]\n",
    "                   .apply( lambda x: 0 if x['promo2aux'] == 0 \n",
    "                                       else 1 if x['month_map'] in x['promo_interval'].split( ',' ) \n",
    "                                       else 0, axis=1 ))\n",
    "\n",
    "# Attribute PROMO2_SINCE\n",
    "# PS: using features: \"promo2\", \"since_week\" and \"since_year\"\n",
    "df2['promo2_since'] = df2['promo2_since_year'].astype( str ) + '-' + df2['promo2_since_week'].astype( str )\n",
    "df2['promo2_since'] = df2['promo2_since'].apply( \n",
    "    lambda x: datetime.datetime.strptime( x + '-1', '%Y-%W-%w' ) - datetime.timedelta( days=7 ) \n",
    ")\n",
    "\n",
    "# Attribute PROMO2_TIME_WEEK\n",
    "df2['promo2_time_week'] = ( ( df2['date'] - df2['promo2_since'] )/7 ).apply( lambda x: x.days ).astype( np.int64 )\n",
    "df2['promo2_time_week'] = df2.apply( lambda x: 0.0 if x['promo2_time_week'] < 0 else x['promo2_time_week'], axis=1 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IS_PROMO2 Example: STORE-28 promo2 after adjusted by promo2_since[week,year]\n",
    "#                    STORE-28 started its 'promo2' only in 2015, 6th week.\n",
    "aux = df2.loc[ df2['store']==28 , ['date','promo2_since','is_promo2','promo2_time_week']]\n",
    "plt.figure( figsize=(10, 1) )\n",
    "#\n",
    "plt.subplot( 2, 1, 1)\n",
    "plt.plot(aux['date'], aux['is_promo2'] );\n",
    "plt.tick_params( axis='x', which='both', bottom=False, top=False, labelbottom=False);\n",
    "plt.annotate( 'is_promo2', xy=(240,60), fontsize=10, xycoords='figure points' );\n",
    "#\n",
    "plt.subplot( 2, 1, 2)\n",
    "plt.plot(aux['date'], aux['promo2_time_week'] );\n",
    "plt.xticks(rotation=0, fontsize=8);\n",
    "plt.annotate( 'promo2_time_week', xy=(220,30), fontsize=10, xycoords='figure points' );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2 Atributes derived from \"Competition\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attribute HAS_COMPETITION: =ZERO in the days before CompetitionOpenSince[month/year].\n",
    "#                            =ONE after that date.\n",
    "df2['has_competition'] = ( df2\n",
    "    .apply( lambda x: 0 if x['date'].year < x['competition_open_since_year'] \n",
    "                        or (x['date'].year == x['competition_open_since_year'] and x['date'].month < x['competition_open_since_month']) \n",
    "                        else 1, axis=1 ) )\n",
    "\n",
    "# Attribute COMPETITION_DISTANCE\n",
    "df2['competition_distance'] = (df2\n",
    "        .apply( lambda x: 200000.0 if x['has_competition'] == 0 \n",
    "                                 else x['competition_distance'], axis=1 ))\n",
    "\n",
    "# Attribute COMPETITION_SINCE\n",
    "# PS: info separated in MONTH & YEAR. Join them and calculate date difference to current date\n",
    "df2['competition_since'] = df2.apply( \n",
    "    lambda x: datetime.datetime( \n",
    "        year=x['competition_open_since_year'], \n",
    "        month=x['competition_open_since_month'], day=1 ), \n",
    "    axis=1 )\n",
    "df2['competition_time_month'] = (( ( df2['date'] - df2['competition_since'] )/30 )\n",
    "        .apply( lambda x: x.days ).astype( np.int64 ))\n",
    "df2['competition_time_month'] = (df2\n",
    "        .apply( lambda x: 0.0 if x['competition_time_month'] < 0 \n",
    "                            else x['competition_time_month'], axis=1 ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: STORE-14  started to face competition in 2014-march\n",
    "#          STORE-122 started to face competition in 2013-april\n",
    "aux = df2.loc[ df2['store']==14 , ['date','sales','has_competition','competition_time_month']]\n",
    "plt.figure( figsize=(10, 1) )\n",
    "plt.subplot( 2, 1, 1)\n",
    "plt.plot(aux['date'], aux['has_competition'] );\n",
    "plt.annotate( 'has_competition', xy=(120,60), fontsize=10, xycoords='figure points' );\n",
    "plt.xticks(rotation=30, fontsize=8);\n",
    "\n",
    "plt.subplot( 2, 1, 2)\n",
    "plt.plot(aux['date'], aux['competition_time_month'] );\n",
    "plt.annotate( 'competition_time_month', xy=(105,30), fontsize=10, xycoords='figure points' );\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: STORE-122 - competition started in 2013-april\n",
    "aux = df2.loc[ df2['store']==122 , ['date','has_competition','competition_distance','competition_time_month']]\n",
    "plt.figure( figsize=(10, 4) )\n",
    "plt.subplot( 3, 1, 1)\n",
    "plt.plot(aux['date'], aux['has_competition'] );\n",
    "plt.tick_params( axis='x', which='both', bottom=False, top=False, labelbottom=False)\n",
    "plt.ylabel('has_competition');\n",
    "#\n",
    "plt.subplot( 3, 1, 2)\n",
    "plt.plot(aux['date'], aux['competition_distance'] );\n",
    "plt.tick_params( axis='x', which='both', bottom=False, top=False, labelbottom=False)\n",
    "plt.ylabel('competition_distance');\n",
    "#\n",
    "plt.subplot( 3, 1, 3)\n",
    "plt.plot(aux['date'], aux['competition_time_month'] );\n",
    "plt.xticks(rotation=0, fontsize=8);\n",
    "plt.ylabel('competition_time_month');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.3 Attributes: time periodicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# year\n",
    "df2['year'] = df2['date'].dt.year.astype( np.int64 )\n",
    "# month\n",
    "df2['month'] = df2['date'].dt.month.astype( np.int64 )\n",
    "# day\n",
    "df2['day'] = df2['date'].dt.day.astype( np.int64 )\n",
    "# semester\n",
    "df2['semester'] = df2['month'].apply( lambda x: 0 if x < 7 else 1 ).astype( np.int64 )\n",
    "# quarter\n",
    "df2['quarter'] = df2['month'].apply( lambda x: 1 if x < 4 \n",
    "                                          else 2 if x < 7 \n",
    "                                          else 3 if x < 10 \n",
    "                                          else 4 ).astype( np.int64 )\n",
    "# two months\n",
    "df2['2months'] = df2['month'].apply( lambda x: 1 if x < 3 \n",
    "                                          else 2 if x < 5 \n",
    "                                          else 3 if x < 7 \n",
    "                                          else 4 if x < 9 \n",
    "                                          else 5 if x < 11 \n",
    "                                          else 6 ).astype( np.int64 )\n",
    "# fortnight\n",
    "df2['fortnight_of_year'] = df2.apply( lambda x: (2* x['month']) if x['day'] > 15 \n",
    "                                           else (2* x['month'] -1), axis=1 ).astype( np.int64 )\n",
    "df2['fortnight_of_month'] = df2.apply( lambda x: 1 if x['day'] > 15 else 0, axis=1)\n",
    "\n",
    "# week of year\n",
    "# \"weekofyear\" deprecated, in favor of \"isocalendar\"\n",
    "# REF: https://pandas.pydata.org/pandas-docs/version/1.5/reference/api/pandas.Series.dt.weekofyear.html\n",
    "# REF: https://pandas.pydata.org/pandas-docs/version/1.5/reference/api/pandas.Series.dt.isocalendar.html#pandas.Series.dt.isocalendar\n",
    "df2['week_of_year'] = df2['date'].dt.isocalendar().week.astype( np.int64 )\n",
    "\n",
    "# year week (string)\n",
    "df2['year_week'] = df2['date'].dt.strftime( '%Y-%W' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.4 Attributes: \"assortment\" & \"state_holiday\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assortment: a=basic, b=extra, c=extended\n",
    "df2['assortment'] = (df2['assortment']\n",
    "                     .apply( lambda x: 'basic' if x == 'a' \n",
    "                                  else 'extra' if x == 'b' \n",
    "                                  else 'extended' ))\n",
    "# state holiday\n",
    "# a=public holiday, b=Easter holiday, c=Christmas, 0=regular working day\n",
    "df2['state_holiday'] = df2['state_holiday'].apply( \n",
    "    lambda x: 'public_holiday' if x == 'a' else \n",
    "              'easter_holiday' if x == 'b' else \n",
    "              'christmas'      if x == 'c' else \n",
    "              'regular_day' )\n",
    "\n",
    "df2[['store','assortment','state_holiday']].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.5 Attribute: \"sales_per_customer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['sales_per_customer'] = (df2\n",
    "                .apply( lambda x: 0 if x['customers'] == 0 \n",
    "                                  else x['sales'] / x['customers'], axis=1 ))\n",
    "\n",
    "df2[['store', 'sales', 'customers', 'sales_per_customer']].sample(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 3. DATA FILTERING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0. Initial Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.copy()\n",
    "df3.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Rows Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If stores are closed (open=0), then SALES=0 and CONSUMERS=0. Let's see:\n",
    "aux1 = df3.loc[:, ['open', 'sales']].copy()\n",
    "aux11 = aux1.groupby('open').sum().reset_index()\n",
    "aux1 = df3.loc[:, ['open', 'customers']].copy()\n",
    "aux12 = aux1.groupby('open').sum().reset_index()\n",
    "aux13 = pd.merge( aux11, aux12, how='inner', on='open' )\n",
    "aux13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1. ROWS FILTERING\n",
    "\n",
    "df3 = df3[(df3['open'] != 0) & (df3['sales'] > 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Columns Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2. COLUMNS SELECTION\n",
    "#\n",
    "# DELETE COLUMNS:\n",
    "# - Del 'OPEN'. After filtering rows, the 'OPEN' column can be deleted.\n",
    "# - The columns 'PROMO_INTERVAL' and 'MONTH_MAP' are no longer needed, because were transformed in other cols.\n",
    "\n",
    "cols_drop = [ 'open', 'promo_interval', 'month_map', 'promo2aux' ]\n",
    "df3 = df3.drop( cols_drop, axis=1 )\n",
    "\n",
    "# 'CUSTOMERS' & 'SALES_PER_CUSTOMER' Columns:\n",
    "# - Both columns should be deleted, since 'customers' won't be available during prediction (=business restriction)\n",
    "# - But it's an important feature in EDA, to evaluate & understand the business\n",
    "# - Result: it will be deleted after step-4 item 4.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.sample(6).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 4. EXPLORATORY DATA ANALYSIS (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0. Initial Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df3.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Univariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aim: understand the characteristics & behaviour of each feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.0. Define Numerical & Categorical Features (as in 1.8.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually Separate: numerical features\n",
    "num_attr = [ 'sales', 'customers', 'sales_per_customer', 'competition_distance', 'promo2_time_week', 'competition_time_month' ]\n",
    "num_attributes = df4.loc[:, num_attr].copy()\n",
    "\n",
    "# Categorical features: all the others\n",
    "cat_attr = []\n",
    "for a in df4.columns:\n",
    "    if a in num_attr:\n",
    "        #print(a)\n",
    "        continue\n",
    "    cat_attr.append(a)\n",
    "cat_attributes = df4.loc[:, cat_attr].copy()\n",
    "\n",
    "num_attributes.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1. Numerical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1.1. DESCRIPTIVE STATISTICS - NUMERICAL FEATURES\n",
    "# Dispersion (variance, standard deviation, max, min, range, skew, kurtosis)\n",
    "# central tendency (mean, median)\n",
    "\n",
    "# Central Tendency - mean, median\n",
    "ct1 = pd.DataFrame( num_attributes.apply( np.mean ) ).T\n",
    "ct2 = pd.DataFrame( num_attributes.apply( np.median ) ).T\n",
    "\n",
    "# Dispersion - std, min, max, range, skew, kurtosis\n",
    "d1 = pd.DataFrame( num_attributes.apply( np.std ) ).T\n",
    "d2 = pd.DataFrame( num_attributes.apply( min ) ).T\n",
    "d3 = pd.DataFrame( num_attributes.apply( max ) ).T\n",
    "d4 = pd.DataFrame( num_attributes.apply( lambda x: x.max() - x.min() ) ).T\n",
    "d5 = pd.DataFrame( num_attributes.apply( lambda x: x.skew() ) ).T\n",
    "d6 = pd.DataFrame( num_attributes.apply( lambda x: x.kurtosis() ) ).T\n",
    "\n",
    "# Concatenate the metrics\n",
    "m = pd.concat( [d2, d3, d4, ct1, ct2, d1, d5, d6] ).round(2).T.reset_index()\n",
    "m.columns = ['attributes', 'min', 'max', 'range', 'mean', 'median', 'std', 'skew', 'kurtosis' ]\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1.1 Plot Numerical Features\n",
    "plt.figure( figsize=(10, 9) )\n",
    "plt.subplot( 3, 2, 1 )\n",
    "sns.distplot( num_attributes['sales'] );\n",
    "plt.xticks( rotation=0, fontsize=7 );\n",
    "plt.yticks( rotation=60, fontsize=7 );\n",
    "plt.subplot( 3, 2, 2 )\n",
    "sns.distplot( num_attributes['customers'] );\n",
    "plt.xticks( rotation=0, fontsize=7 );\n",
    "plt.yticks( rotation=60, fontsize=7 );\n",
    "plt.subplot( 3, 2, 3 )\n",
    "sns.distplot( num_attributes['sales_per_customer'] );\n",
    "plt.xticks( rotation=0, fontsize=7 );\n",
    "plt.yticks( rotation=60, fontsize=7 );\n",
    "plt.subplot( 3, 2, 4 )\n",
    "sns.distplot( num_attributes['competition_distance'] );\n",
    "plt.xticks( rotation=0, fontsize=7 );\n",
    "plt.yticks( rotation=60, fontsize=7 );\n",
    "plt.subplot( 3, 2, 5 )\n",
    "sns.distplot( num_attributes['promo2_time_week'] );\n",
    "plt.xticks( rotation=0, fontsize=7 );\n",
    "plt.yticks( rotation=60, fontsize=7 );\n",
    "plt.subplot( 3, 2, 6 )\n",
    "sns.distplot( num_attributes['competition_time_month'] );\n",
    "plt.xticks( rotation=0, fontsize=7 );\n",
    "plt.yticks( rotation=60, fontsize=7 );\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (i) Sales - Response Variable (original & log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 UNIVARIATE ANALYSIS: Response Variable (original & log)\n",
    "plt.figure( figsize=(10, 4) )\n",
    "plt.subplot( 1, 2, 1 )\n",
    "sns.distplot( df4['sales'] );\n",
    "plt.xticks( fontsize=7 );\n",
    "plt.yticks( rotation=60, fontsize=7 );\n",
    "plt.subplot( 1, 2, 2 )\n",
    "sns.distplot( np.log1p( df4['sales'] ) );\n",
    "plt.xticks( fontsize=7 );\n",
    "plt.yticks( fontsize=7 );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (ii) Customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Competition Distance (original & log)\n",
    "plt.figure( figsize=(10, 4) )\n",
    "plt.subplot( 1, 2, 1 )\n",
    "sns.distplot( num_attributes['customers'] );\n",
    "plt.xticks( fontsize=7 );\n",
    "plt.yticks( rotation=60, fontsize=7 );\n",
    "plt.subplot( 1, 2, 2 )\n",
    "sns.distplot( np.log1p( num_attributes['customers'] ) );\n",
    "plt.xticks( fontsize=7 );\n",
    "plt.yticks( fontsize=7 );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (iii) Sales per Customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Competition Distance (original & log)\n",
    "plt.figure( figsize=(10, 4) )\n",
    "plt.subplot( 1, 2, 1 )\n",
    "sns.distplot( num_attributes['sales_per_customer'] );\n",
    "plt.xticks( fontsize=7 );\n",
    "plt.yticks( rotation=60, fontsize=7 );\n",
    "plt.subplot( 1, 2, 2 )\n",
    "sns.distplot( np.log1p( num_attributes['sales_per_customer'] ) );\n",
    "plt.xticks( fontsize=7 );\n",
    "plt.yticks( fontsize=7 );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (iv) Competition Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Competition Distance (original & log)\n",
    "plt.figure( figsize=(10, 4) )\n",
    "plt.subplot( 1, 2, 1 )\n",
    "sns.distplot( num_attributes['competition_distance'] );\n",
    "plt.xticks( fontsize=7 );\n",
    "plt.yticks( rotation=60, fontsize=7 );\n",
    "plt.subplot( 1, 2, 2 )\n",
    "sns.distplot( np.log1p( num_attributes['competition_distance'] ) );\n",
    "plt.xticks( fontsize=7 );\n",
    "plt.yticks( fontsize=7 );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (v) promo2_time_week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Competition Distance (original & log)\n",
    "plt.figure( figsize=(10, 3) )\n",
    "plt.subplot( 1, 2, 1 )\n",
    "sns.histplot( num_attributes['promo2_time_week'] );\n",
    "plt.xticks( fontsize=7 );\n",
    "plt.yticks( rotation=60, fontsize=7 );\n",
    "plt.subplot( 1, 2, 2 )\n",
    "sns.histplot( np.log1p( num_attributes['promo2_time_week'] ) );\n",
    "plt.xticks( fontsize=7 );\n",
    "plt.yticks( fontsize=7 );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (vi) competition_time_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Competition Distance (original & log)\n",
    "plt.figure( figsize=(10, 3) )\n",
    "plt.subplot( 1, 2, 1 )\n",
    "sns.histplot( num_attributes['competition_time_month'] );\n",
    "plt.xticks( fontsize=7 );\n",
    "plt.yticks( rotation=60, fontsize=7 );\n",
    "plt.subplot( 1, 2, 2 )\n",
    "sns.histplot( np.log1p( num_attributes['competition_time_month'] ) );\n",
    "plt.xticks( fontsize=7 );\n",
    "plt.yticks( fontsize=7 );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2. Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting categorical features\n",
    "cols_to_plot = ['day_of_week', 'promo', 'state_holiday', 'school_holiday',\n",
    "       'store_type', 'assortment', 'competition_open_since_month', \n",
    "       'competition_open_since_year', 'promo2', 'promo2_since_week', \n",
    "       'promo2_since_year', 'is_promo2', 'has_competition' ]\n",
    "\n",
    "cat_attributes.hist( column=cols_to_plot, bins=25, figsize=(10,10), color='red' );\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_attributes.apply( lambda x: x.unique().shape[0] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### state_holiday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1.2 Categorical Features Analysis: state_holiday\n",
    "\n",
    "plt.figure( figsize=(10, 4) )\n",
    "plt.subplot( 1, 2, 1 )\n",
    "a = df4[ df4['state_holiday'] != 'regular_day' ]\n",
    "sns.countplot( x='state_holiday', data=a )\n",
    "\n",
    "plt.subplot( 1, 2, 2 )\n",
    "sns.kdeplot( df4[df4['state_holiday'] == 'public_holiday']['sales'], label='public_holiday', fill=True );\n",
    "sns.kdeplot( df4[df4['state_holiday'] == 'easter_holiday']['sales'], label='easter_holiday', fill=True );\n",
    "sns.kdeplot( df4[df4['state_holiday'] == 'christmas']['sales'], label='christmas', fill=True );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### store_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1.2 Categorical Features Analysis: store_type\n",
    "\n",
    "plt.figure( figsize=(14, 4) )\n",
    "plt.subplot( 1, 2, 1 )\n",
    "sns.countplot( x='store_type', data=df4 )\n",
    "\n",
    "plt.subplot( 1, 2, 2 )\n",
    "sns.kdeplot( df4[df4['store_type'] == 'a']['sales'], label='a', fill=True )\n",
    "sns.kdeplot( df4[df4['store_type'] == 'b']['sales'], label='b', fill=True )\n",
    "sns.kdeplot( df4[df4['store_type'] == 'c']['sales'], label='c', fill=True )\n",
    "sns.kdeplot( df4[df4['store_type'] == 'd']['sales'], label='d', fill=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### assortment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4['assortment'].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1.2 Categorical Features Analysis: assortment\n",
    "\n",
    "plt.figure( figsize=(14, 4) )\n",
    "plt.subplot( 1, 2, 1 )\n",
    "sns.countplot( x='assortment', data=df4 )\n",
    "\n",
    "plt.subplot( 1, 2, 2 )\n",
    "sns.kdeplot( df4[df4['assortment'] == 'basic']['sales'], label='basic', fill=True )\n",
    "sns.kdeplot( df4[df4['assortment'] == 'extended']['sales'], label='extended', fill=True )\n",
    "sns.kdeplot( df4[df4['assortment'] == 'extra']['sales'], label='extra', fill=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Bivariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aim: understand the impact of each attribute to the response variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.0 Total Daily Averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_Sales_tot = df4.loc[:, 'sales'].mean().round(2)\n",
    "avg_Customers_tot = df4.loc[:, 'customers'].mean().round(2)\n",
    "avg_SalesPerCustomer_tot = df4.loc[:, 'sales_per_customer'].mean().round(2)\n",
    "\n",
    "mdn_Sales_tot = df4.loc[:, 'sales'].median()\n",
    "mdn_Customers_tot = df4.loc[:, 'customers'].median()\n",
    "mdn_SalesPerCustomer_tot = df4.loc[:, 'sales_per_customer'].median()\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'avg_Sales_tot':avg_Sales_tot,\n",
    "    'mdn_Sales_tot':mdn_Sales_tot,\n",
    "    'avg_Customers_tot':avg_Customers_tot,\n",
    "    'mdn_Customers_tot':mdn_Customers_tot,\n",
    "    'avg_SalesPerCustomer_tot':avg_SalesPerCustomer_tot,\n",
    "    'mdn_SalesPerCustomer_tot':mdn_SalesPerCustomer_tot\n",
    "}, index=[0])\n",
    "\n",
    "df.T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 Sazonalities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Day of Week (important)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tabulated data and plottings bellow indicate the following:\n",
    "1. SUNDAYS (weekday=7): differ from the other days, due to havving only 33 stores open. These 33 stores should be prepared to receive a duplicated amount of consumers (in average), and each one willing to buy cheaper products, since just 2/3 of the sales (in median) occur on Sundays.\n",
    "2. MONDAYS (weekday=1): the selling statistics are stronger, considering the higher total daily sales (in median), the higher number of daily sales per customer (in median), and the second higher number of daily customers (in median).\n",
    "3. SATURDAYS (weekday=6): the selling statistics are the weakest ones, considering the lowest number in average sales and average number of customers. Nevertheless, that may be due to the reduced working hours, from 9 a.m. to 1 p.m., according to \"https://www.bonn.de/microsite/en/services/medical-care/pharmacies.php#:~:text=A%20pharmacy%20in%20Germany%20is,to%201%20p.m.%20on%20Saturdays.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N# of unique stores open per weekday\n",
    "num_open_stores = df4.loc[:, ['day_of_week', 'store']].groupby('day_of_week').nunique().reset_index()\n",
    "# Avg Sales per weekday\n",
    "Avg_Sales = df4.loc[:, ['day_of_week', 'sales']].groupby('day_of_week').mean().reset_index()\n",
    "# Median Sales per weekday\n",
    "Mdn_Sales = df4.loc[:, ['day_of_week', 'sales']].groupby('day_of_week').median().reset_index()\n",
    "# Avg Customers per weekday\n",
    "Avg_Customers = df4.loc[:, ['day_of_week', 'customers']].groupby('day_of_week').mean().reset_index()\n",
    "# Median Customers per weekday\n",
    "Mdn_Customers = df4.loc[:, ['day_of_week', 'customers']].groupby('day_of_week').median().reset_index()\n",
    "# Median Sales per Customer per weekday\n",
    "Mdn_SalesPerCustomer = df4.loc[:, ['day_of_week', 'sales_per_customer']].groupby('day_of_week').median().reset_index()\n",
    "\n",
    "# drop col\n",
    "Avg_Sales.drop('day_of_week', inplace=True, axis=1)\n",
    "Mdn_Sales.drop('day_of_week', inplace=True, axis=1)\n",
    "Avg_Customers.drop('day_of_week', inplace=True, axis=1)\n",
    "Mdn_Customers.drop('day_of_week', inplace=True, axis=1)\n",
    "Mdn_SalesPerCustomer.drop('day_of_week', inplace=True, axis=1)\n",
    "\n",
    "# Concat metrics\n",
    "week_metrics = pd.concat( [num_open_stores.T, Avg_Sales.T, Mdn_Sales.T, Avg_Customers.T, Mdn_Customers.T, Mdn_SalesPerCustomer.T] ).round(2).T\n",
    "week_metrics.columns = [ 'day.of.week', 'open.stores', 'avg.sales', 'mdn.sales', 'avg.customers', 'mdn.customers', 'mdn.sales/customer' ]\n",
    "\n",
    "\n",
    "# Plottings\n",
    "plt.figure( figsize=(10, 4) )\n",
    "plt.subplot( 1, 3, 1)\n",
    "sns.boxplot( x='day_of_week', y='sales', data=df4 );\n",
    "plt.title(\"SALES\");\n",
    "plt.subplot( 1, 3, 2)\n",
    "sns.boxplot( x='day_of_week', y='customers', data=df4 );\n",
    "plt.title(\"CUSTOMERS\");\n",
    "plt.subplot( 1, 3, 3)\n",
    "sns.boxplot( x='day_of_week', y='sales_per_customer', data=df4 );\n",
    "plt.title(\"SALES/CUSTOMERS\");\n",
    "\n",
    "week_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Month (important)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tabulated data and plottings bellow indicate the following:\n",
    "1. DECEMBER: Revenue impacted by Christmas. The selling statistics are the strongest, considering daily sales, number of customers, and sales per customer.\n",
    "2. NOVEMBER: Second stronger month, possibly caused by Black Friday Sales.\n",
    "3. MAY: Third stronger month, probably caused by Mother's Day.\n",
    "4. SEPTEMBER, JANUARY & FEBRUARY: these months show the lowest selling statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avg & Median Sales per month\n",
    "Avg_Sales_Month = df4.loc[:, ['month','sales']].groupby('month').mean().reset_index()\n",
    "Mdn_Sales_Month = df4.loc[:, ['month','sales']].groupby('month').median().reset_index()\n",
    "# Avg & Median Customers per month\n",
    "Avg_Customers_Month = df4.loc[:, ['month','customers']].groupby('month').mean().reset_index()\n",
    "Mdn_Customers_Month = df4.loc[:, ['month','customers']].groupby('month').median().reset_index()\n",
    "# Avg & Median Customers per Sales per month\n",
    "Avg_SalesPerCustomer_Month = df4.loc[:, ['month','sales_per_customer']].groupby('month').mean().reset_index()\n",
    "Mdn_SalesPerCustomer_Month = df4.loc[:, ['month','sales_per_customer']].groupby('month').median().reset_index()\n",
    "# drop col...\n",
    "Mdn_Sales_Month.drop('month', inplace=True, axis=1)\n",
    "Avg_Customers_Month.drop('month', inplace=True, axis=1)\n",
    "Mdn_Customers_Month.drop('month', inplace=True, axis=1)\n",
    "Avg_SalesPerCustomer_Month.drop('month', inplace=True, axis=1)\n",
    "Mdn_SalesPerCustomer_Month.drop('month', inplace=True, axis=1)\n",
    "# Concat metrics\n",
    "month_metrics = pd.concat([Avg_Sales_Month.T, Mdn_Sales_Month.T, \n",
    "                           Avg_Customers_Month.T, Mdn_Customers_Month.T, \n",
    "                           Avg_SalesPerCustomer_Month.T, Mdn_SalesPerCustomer_Month.T]).round(2).T\n",
    "month_metrics.columns = ['month','avg.Sales','mdn.Sales','avg.Customers','mdn.Customers','avg.Sales/Custm','mdn.Sales/Custm']\n",
    "\n",
    "# plotting\n",
    "plt.figure( figsize=(10, 4) )\n",
    "plt.scatter(month_metrics['month'], month_metrics['mdn.Sales'] );\n",
    "plt.xticks(rotation=0, fontsize=8);\n",
    "plt.title('Median sales by month');\n",
    "\n",
    "# Boxplot\n",
    "plt.figure( figsize=(10, 4) )\n",
    "plt.subplot( 1, 3, 1)\n",
    "sns.boxplot( x='month', y='sales', data=df4 );\n",
    "plt.title(\"SALES\");\n",
    "plt.subplot( 1, 3, 2)\n",
    "sns.boxplot( x='month', y='customers', data=df4 );\n",
    "plt.title(\"CUSTOMERS\");\n",
    "plt.subplot( 1, 3, 3)\n",
    "sns.boxplot( x='month', y='sales_per_customer', data=df4 );\n",
    "plt.title(\"SALES/CUSTOMERS\");\n",
    "\n",
    "# Table\n",
    "month_metrics.sort_values('avg.Sales', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Day (important)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tabulated data and plottings bellow indicate the following:\n",
    "1. There is a clear seasonal behaviour, showing two periods of peak and two periods of valey.\n",
    "2. The strongest peak occur from the 29th up to the 4th or 5th of the following month, possibly due to salary payments in the economy.\n",
    "3. Another peak occur in the 3rd week, between the 16th and 19th\n",
    "4. The weakest periods occur between the 10th and 13th, and again in two weeks, between the 24th and 26th."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the mean & sum of Sales per days\n",
    "AvgSales = df4.loc[:, ['day', 'sales']].groupby('day').mean().reset_index()\n",
    "AvgSales.columns = [ 'day', 'avg_sales' ]\n",
    "MdnSales = df4.loc[:, ['day', 'sales']].groupby('day').median().reset_index()\n",
    "MdnSales.columns = [ 'day', 'median_sales' ]\n",
    "# Join it all\n",
    "df_sales_day = pd.merge( AvgSales, MdnSales, how='inner', on='day' ).sort_values('median_sales', ascending=False).round(0)\n",
    "# plotting\n",
    "plt.figure( figsize=(10, 4) )\n",
    "plt.scatter(df_sales_day['day'], df_sales_day['median_sales'] );\n",
    "plt.xticks(rotation=0, fontsize=8);\n",
    "plt.title('Median Sales during one month');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Week of Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the mean & sum of Sales per week_of_year\n",
    "aux11 = df4.loc[:, ['week_of_year', 'sales']].copy()\n",
    "AvgSales = aux11.groupby('week_of_year').mean().reset_index()\n",
    "AvgSales.columns = [ 'week_of_year', 'avg_sales' ]\n",
    "MdnSales = aux11.groupby('week_of_year').median().reset_index()\n",
    "MdnSales.columns = [ 'week_of_year', 'median_sales' ]\n",
    "SumSales = aux11.groupby('week_of_year').sum().reset_index()\n",
    "SumSales.columns = [ 'week_of_year', 'sum_sales' ]\n",
    "\n",
    "# Join it all\n",
    "df_sales_weekyear = pd.merge( AvgSales, MdnSales, how='inner', on='week_of_year' )\n",
    "df_sales_weekyear = pd.merge( df_sales_weekyear, SumSales, how='inner', on='week_of_year' ).sort_values('median_sales', ascending=False).round(0)\n",
    "\n",
    "# Boxplot\n",
    "plt.figure( figsize=(10, 6) )\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.boxplot( x='week_of_year', y='sales', data=df4 );\n",
    "\n",
    "# Scatter Plotting\n",
    "plt.figure( figsize=(10, 4) )\n",
    "plt.scatter(df_sales_weekyear['week_of_year'], df_sales_weekyear['median_sales'] );\n",
    "plt.xticks(rotation=0, fontsize=8);\n",
    "plt.title('Median Sales during one year - by week');\n",
    "\n",
    "# Barplot\n",
    "plt.figure(figsize=(10,4))\n",
    "sns.barplot(df4, x='week_of_year', y='sales');\n",
    "plt.title('Sales per Week of Year');\n",
    "plt.xlabel(\"WEEK OF YEAR\");\n",
    "plt.ylabel(\"SALES\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quarter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tabulated data and plottings bellow indicate the following:\n",
    "1. The 4th quarter shows the strongest sales, which is probably related to the occurences of Christmas and Black Friday.\n",
    "2. The 2nd quarter is the second best of the year, being probably related to Mother's Day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the mean & sum of Sales per Quarter\n",
    "aux11 = df4.loc[:, ['quarter', 'sales']].copy()\n",
    "AvgSales = aux11.groupby('quarter').mean().reset_index()\n",
    "AvgSales.columns = [ 'quarter', 'avg_sales' ]\n",
    "MdnSales = aux11.groupby('quarter').median().reset_index()\n",
    "MdnSales.columns = [ 'quarter', 'median_sales' ]\n",
    "SumSales = aux11.groupby('quarter').sum().reset_index()\n",
    "SumSales.columns = [ 'quarter', 'sum_sales' ]\n",
    "# Join it all\n",
    "df_sales_quarter = pd.merge( AvgSales, MdnSales, how='inner', on='quarter' )\n",
    "df_sales_quarter = pd.merge( df_sales_quarter, SumSales, how='inner', on='quarter' ).sort_values('median_sales', ascending=False).round(0)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10,4))\n",
    "sns.barplot(df4, x='quarter', y='sales');\n",
    "plt.title('Sales per Quarter of Year');\n",
    "plt.xlabel(\"QUARTER\");\n",
    "plt.ylabel(\"SALES\");\n",
    "\n",
    "df_sales_quarter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Semester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the mean & sum of Sales per semester\n",
    "aux11 = df4.loc[:, ['semester', 'sales']].copy()\n",
    "AvgSales = aux11.groupby('semester').mean().reset_index()\n",
    "AvgSales.columns = [ 'semester', 'avg_sales' ]\n",
    "MdnSales = aux11.groupby('semester').median().reset_index()\n",
    "MdnSales.columns = [ 'semester', 'median_sales' ]\n",
    "SumSales = aux11.groupby('semester').sum().reset_index()\n",
    "SumSales.columns = [ 'semester', 'sum_sales' ]\n",
    "# Join it all\n",
    "df_sales_semester = pd.merge( AvgSales, MdnSales, how='inner', on='semester' )\n",
    "df_sales_semester = pd.merge( df_sales_semester, SumSales, how='inner', on='semester' ).sort_values('median_sales', ascending=False).round(0)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10,4))\n",
    "sns.barplot(df4, x='semester', y='sales')\n",
    "plt.title('Sales per Semester')\n",
    "plt.xlabel(\"SEMESTER\")\n",
    "plt.ylabel(\"SALES\")\n",
    "\n",
    "df_sales_semester"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Two Months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the mean & sum of Sales per 2months\n",
    "aux11 = df4.loc[:, ['2months', 'sales']].copy()\n",
    "AvgSales = aux11.groupby('2months').mean().reset_index()\n",
    "AvgSales.columns = [ '2months', 'avg_sales' ]\n",
    "MdnSales = aux11.groupby('2months').median().reset_index()\n",
    "MdnSales.columns = [ '2months', 'median_sales' ]\n",
    "SumSales = aux11.groupby('2months').sum().reset_index()\n",
    "SumSales.columns = [ '2months', 'sum_sales' ]\n",
    "# Join it all\n",
    "df_sales_2month = pd.merge( AvgSales, MdnSales, how='inner', on='2months' )\n",
    "df_sales_2month = pd.merge( df_sales_2month, SumSales, how='inner', on='2months' ).sort_values('median_sales', ascending=False).round(0)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10,4))\n",
    "sns.barplot(df4, x='2months', y='sales');\n",
    "plt.title('Sales per 2-Months of Year');\n",
    "plt.xlabel(\"2-MONTHS\");\n",
    "plt.ylabel(\"SALES\");\n",
    "\n",
    "# Boxplot\n",
    "plt.figure( figsize=(10, 6) )\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.boxplot( x='2months', y='sales', data=df4 );\n",
    "\n",
    "df_sales_2month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fortnight of Year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- No big benefit, compared to monthly sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the mean & sum of Sales per fortnight_of_year\n",
    "aux11 = df4.loc[:, ['fortnight_of_year', 'sales']].copy()\n",
    "AvgSales = aux11.groupby('fortnight_of_year').mean().reset_index()\n",
    "AvgSales.columns = [ 'fortnight_of_year', 'avg_sales' ]\n",
    "MdnSales = aux11.groupby('fortnight_of_year').median().reset_index()\n",
    "MdnSales.columns = [ 'fortnight_of_year', 'median_sales' ]\n",
    "SumSales = aux11.groupby('fortnight_of_year').sum().reset_index()\n",
    "SumSales.columns = [ 'fortnight_of_year', 'sum_sales' ]\n",
    "\n",
    "# Join it all\n",
    "df_sales_fortny = pd.merge( AvgSales, MdnSales, how='inner', on='fortnight_of_year' )\n",
    "df_sales_fortny = pd.merge( df_sales_fortny, SumSales, how='inner', on='fortnight_of_year' ).sort_values('median_sales', ascending=False).round(0)\n",
    "\n",
    "# Boxplot\n",
    "plt.figure( figsize=(10, 6) )\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.boxplot( x='fortnight_of_year', y='sales', data=df4 );\n",
    "\n",
    "# Scatter Plotting\n",
    "plt.figure( figsize=(10, 4) )\n",
    "plt.scatter(df_sales_fortny['fortnight_of_year'], df_sales_fortny['median_sales'] );\n",
    "plt.xticks(rotation=0, fontsize=8);\n",
    "plt.title('Median Sales during one year - by fortnight');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fortnight of Month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- No strong information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the mean & sum of Sales per fortnight_of_month\n",
    "aux11 = df4.loc[:, ['fortnight_of_month', 'sales']].copy()\n",
    "AvgSales = aux11.groupby('fortnight_of_month').mean().reset_index()\n",
    "AvgSales.columns = [ 'fortnight_of_month', 'avg_sales' ]\n",
    "MdnSales = aux11.groupby('fortnight_of_month').median().reset_index()\n",
    "MdnSales.columns = [ 'fortnight_of_month', 'median_sales' ]\n",
    "SumSales = aux11.groupby('fortnight_of_month').sum().reset_index()\n",
    "SumSales.columns = [ 'fortnight_of_month', 'sum_sales' ]\n",
    "\n",
    "# Join it all\n",
    "df_sales_fortnm = pd.merge( AvgSales, MdnSales, how='inner', on='fortnight_of_month' )\n",
    "df_sales_fortnm = pd.merge( df_sales_fortnm, SumSales, how='inner', on='fortnight_of_month' ).sort_values('median_sales', ascending=False).round(0)\n",
    "df_sales_fortnm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2. Store Type (important)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) Total Base Indicators (important)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table information & plottings bellow indicate that:\n",
    "1. TYPE-B Store: these 17 units can be called POPUPAR STORES, since they are prepared to receive more than 2.000 customers a day, about 2,5x the frequency of customers the other units attend. Besides that, the value of the products purchased by customers in TYPE-B stores is almost half the value occurred on the other stores, indicating these TYPE-B stores deal with less expensive products (or: popular products)\n",
    "2. TYPE-D: these 348 units can be called SOFISTICATED STORES, since they deal with most expensive products.\n",
    "3. TYPES A & C: these 750 units can be called NORMAL STORES, due to the higher number of these stores and the medium priced products they deal. On the other hand, the dataset information does not allow us to differentiate stores A & C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "# Sales vs Store-type\n",
    "plt.subplot(1,3,1)\n",
    "sns.barplot(df4, x='store_type', y='sales', order='abcd');\n",
    "plt.title('Total Daily Sales');\n",
    "plt.xlabel(\"STORE TYPE\");\n",
    "plt.ylabel(\"SALES\");\n",
    "# Customers per Store-type\n",
    "plt.subplot(1,3,2)\n",
    "sns.barplot(df4, x='store_type', y='customers', order='abcd');\n",
    "plt.title('N# of Daily Customers');\n",
    "plt.xlabel(\"STORE TYPE\");\n",
    "plt.ylabel(\"SALES\");\n",
    "# Sales/Customer per Store-type\n",
    "plt.subplot(1,3,3)\n",
    "sns.barplot(df4, x='store_type', y='sales_per_customer', order='abcd');\n",
    "plt.title('Daily Value purchased by Customers');\n",
    "plt.xlabel(\"STORE TYPE\");\n",
    "plt.ylabel(\"SALES\");\n",
    "\n",
    "# N# of unique stores per type\n",
    "num_stores_type = df4.loc[:, ['store_type', 'store']].groupby('store_type').nunique().reset_index()\n",
    "num_stores_type.columns = ['store_type', 'Qtty']\n",
    "num_stores_type.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) Store-Types on Sundays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table information & plottings bellow indicate that:\n",
    "1. All 17 TYPE-B stores are open on sundays, receiving the same number of costumers (in average)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux = df4.loc[ df4['day_of_week']==7 ].copy()\n",
    "plt.figure(figsize=(12,4))\n",
    "# Sales vs Store-type\n",
    "plt.subplot(1,3,1)\n",
    "sns.barplot(aux, x='store_type', y='sales', order='abcd');\n",
    "plt.title('Daily Sales on Sunday');\n",
    "plt.xlabel(\"STORE TYPE\");\n",
    "plt.ylabel(\"SALES\");\n",
    "# Customers per Store-type\n",
    "plt.subplot(1,3,2)\n",
    "sns.barplot(aux, x='store_type', y='customers', order='abcd');\n",
    "plt.title('N# Daily Customers on Sunday');\n",
    "plt.xlabel(\"STORE TYPE\");\n",
    "plt.ylabel(\"SALES\");\n",
    "# Sales/Customer per Store-type\n",
    "plt.subplot(1,3,3)\n",
    "sns.barplot(aux, x='store_type', y='sales_per_customer', order='abcd');\n",
    "plt.title('Value purchased on Sunday');\n",
    "plt.xlabel(\"STORE TYPE\");\n",
    "plt.ylabel(\"SALES\");\n",
    "\n",
    "# N# of unique stores per type\n",
    "num_stores_type = aux.loc[:, ['store_type', 'store']].groupby('store_type').nunique().reset_index()\n",
    "num_stores_type.columns = ['store_type', 'Qtty']\n",
    "num_stores_type.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) Store-Types from September to December"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The table information & plottings bellow indicate that, from september (weak month) to december (strong month) all types of stores have increments on SALES, N# OF CUSTOMERS and VALUE PURCHASED."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux = df4.loc[ df4['month']==12 ].copy()\n",
    "plt.figure(figsize=(12,8))\n",
    "# Sales vs Store-type\n",
    "plt.subplot(2,3,1)\n",
    "sns.barplot(aux, x='store_type', y='sales', order='abcd');\n",
    "plt.title('Daily Sales in December');\n",
    "plt.xlabel(\"STORE TYPE\");\n",
    "plt.ylabel(\"SALES\");\n",
    "# Customers per Store-type\n",
    "plt.subplot(2,3,2)\n",
    "sns.barplot(aux, x='store_type', y='customers', order='abcd');\n",
    "plt.title('N# Daily Customers in December');\n",
    "plt.xlabel(\"STORE TYPE\");\n",
    "plt.ylabel(\"SALES\");\n",
    "# Sales/Customer per Store-type\n",
    "plt.subplot(2,3,3)\n",
    "sns.barplot(aux, x='store_type', y='sales_per_customer', order='abcd');\n",
    "plt.title('Value purchased in December');\n",
    "plt.xlabel(\"STORE TYPE\");\n",
    "plt.ylabel(\"SALES\");\n",
    "\n",
    "aux = df4.loc[ df4['month']==9 ].copy()\n",
    "# Sales vs Store-type\n",
    "plt.subplot(2,3,4)\n",
    "sns.barplot(aux, x='store_type', y='sales', order='abcd');\n",
    "plt.title('Daily Sales in September');\n",
    "plt.xlabel(\"STORE TYPE\");\n",
    "plt.ylabel(\"SALES\");\n",
    "# Customers per Store-type\n",
    "plt.subplot(2,3,5)\n",
    "sns.barplot(aux, x='store_type', y='customers', order='abcd');\n",
    "plt.title('N# Daily Customers in September');\n",
    "plt.xlabel(\"STORE TYPE\");\n",
    "plt.ylabel(\"SALES\");\n",
    "# Sales/Customer per Store-type\n",
    "plt.subplot(2,3,6)\n",
    "sns.barplot(aux, x='store_type', y='sales_per_customer', order='abcd');\n",
    "plt.title('Value purchased in September');\n",
    "plt.xlabel(\"STORE TYPE\");\n",
    "plt.ylabel(\"SALES\");\n",
    "\n",
    "# N# of unique stores per type\n",
    "num_stores_type = aux.loc[:, ['store_type', 'store']].groupby('store_type').nunique().reset_index()\n",
    "num_stores_type.columns = ['store_type', 'Qtty']\n",
    "num_stores_type.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3. Assortment (important)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) Total Base Indicators (important)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table information & plottings bellow indicate that:\n",
    "1. Basic-Assortment & Extended-Assortment Stores represent more than 99% of total units.\n",
    "2. Extended-Assortment Stores: represent 46% of the total units. Each unit sells to ~752 customers per day (in average), each sell costing ~ 10 euros.\n",
    "3. Basic-Assortment Stores: represent 53% of the total units. Each unit sells to ~748 customers per day (in average), each sell costing ~ 9 euros.\n",
    "4. Extra-Assortment Stores: represent less than 1% of the total units. Each of its 9 units sells to ~2.068 customers per day (in average), each sell costing ~ 4 euros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,3))\n",
    "# Sales vs Assortment\n",
    "plt.subplot(1,3,1)\n",
    "b1 = sns.barplot(df4, x='assortment', y='sales');\n",
    "for i in b1.containers:\n",
    "    b1.bar_label(i,)\n",
    "plt.title('Daily Sales - All Stores');\n",
    "plt.xlabel(\"ASSORTMENT\");\n",
    "plt.ylabel(\"SALES\");\n",
    "# Customers per Assortment\n",
    "plt.subplot(1,3,2)\n",
    "b2 = sns.barplot(df4, x='assortment', y='customers');\n",
    "for i in b2.containers:\n",
    "    b2.bar_label(i,)\n",
    "plt.title('N# of Daily Customers');\n",
    "plt.xlabel(\"ASSORTMENT\");\n",
    "plt.ylabel(\" \");\n",
    "# Sales/Customer per Assortment\n",
    "plt.subplot(1,3,3)\n",
    "b3 = sns.barplot(df4, x='assortment', y='sales_per_customer');\n",
    "for i in b3.containers:\n",
    "    b3.bar_label(i,)\n",
    "plt.title('Daily Value purchased by Customers');\n",
    "plt.xlabel(\"ASSORTMENT\");\n",
    "plt.ylabel(\" \");\n",
    "\n",
    "assortment = df4.loc[:, ['assortment', 'store']].groupby('assortment').nunique().reset_index()\n",
    "assortment.columns = ['assortment', 'Qtty']\n",
    "assortment.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) Assortment & Story-Type (important)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table information & plottings bellow indicate that:\n",
    "1. BASIC-ASSORTMENT STORES: for types A, C & D basic-assortment stores, the values of purchases per customer are (in average) reduced by less than 2%, compared to the result of all stores. On the other hand, the 7 type-B basic-assortment stores have purchases per customer 22,4% higher, compared to the results of all stores.\n",
    "2. EXTENDED-ASSORTMENT STORES: all these 513 stores have average daily sales ~5,8% higher than the other stores. All them have higher purchases per customer, compared to the results of the whole company.\n",
    "3. There is only one Type-B Extended-Assortment Store (n# 562). It sells to ~3.105 customers per day (in average), each buying ~ 5,76 euros (average).\n",
    "4. EXTRA-ASSORTMENT STORES: these 9 units are all \"Type-B\" Stores. They represent less than 1% of the total units. They sell to ~2.068 customers per day (in average), each customer buying ~ 4,16 euros (average). PS: They all open on Sundays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_assortment( inAssortment: str, df4 ):\n",
    "    if inAssortment=='':\n",
    "        aux = df4.copy()\n",
    "        strName = \"TOTAL\"\n",
    "    else:\n",
    "        aux = df4.loc[ df4['assortment']==inAssortment ].copy()\n",
    "        strName = inAssortment.upper()\n",
    "    plt.figure(figsize=(12,3))\n",
    "    # Sales vs Store-type\n",
    "    plt.subplot(1,3,1)\n",
    "    b1 = sns.barplot(aux, x='store_type', y='sales', order='abcd');\n",
    "    for i in b1.containers:\n",
    "        b1.bar_label(i,)\n",
    "    plt.title('Daily Sales - '+strName);\n",
    "    plt.xlabel(\"STORE TYPE\");\n",
    "    plt.yticks( fontsize=8 );\n",
    "    # Customers per Store-type\n",
    "    plt.subplot(1,3,2)\n",
    "    b2 = sns.barplot(aux, x='store_type', y='customers', order='abcd');\n",
    "    for i in b2.containers:\n",
    "        b2.bar_label(i,)\n",
    "    plt.title('N# Daily Customers - '+strName);\n",
    "    plt.yticks( fontsize=8 );\n",
    "    plt.xlabel(\"STORE TYPE\");\n",
    "    # Sales/Customer per Store-type\n",
    "    plt.subplot(1,3,3)\n",
    "    b3 = sns.barplot(aux, x='store_type', y='sales_per_customer', order='abcd');\n",
    "    for i in b3.containers:\n",
    "        b3.bar_label(i,)\n",
    "    plt.title('Value purchased - '+strName);\n",
    "    plt.yticks( fontsize=8 );\n",
    "    plt.xlabel(\"STORE TYPE\");\n",
    "\n",
    "ta = df4.loc[ df4['store_type']=='a' , ['assortment', 'store']].groupby('assortment').nunique().reset_index()\n",
    "tb = df4.loc[ df4['store_type']=='b' , ['assortment', 'store']].groupby('assortment').nunique().reset_index()\n",
    "tc = df4.loc[ df4['store_type']=='c' , ['assortment', 'store']].groupby('assortment').nunique().reset_index()\n",
    "td = df4.loc[ df4['store_type']=='d' , ['assortment', 'store']].groupby('assortment').nunique().reset_index()\n",
    "tot = df4.loc[ : , ['assortment', 'store']].groupby('assortment').nunique().reset_index()\n",
    "ta.columns = ['assortment', 'Store-A']\n",
    "tb.columns = ['assortment', 'Store-B']\n",
    "tc.columns = ['assortment', 'Store-C']\n",
    "td.columns = ['assortment', 'Store-D']\n",
    "tot.columns = ['assortment', 'Total']\n",
    "abcd = pd.merge( ta, tb, how='right', on='assortment' )\n",
    "abcd = pd.merge( abcd, tc, how='left', on='assortment' )\n",
    "abcd = pd.merge( abcd, td, how='left', on='assortment' )\n",
    "abcd = pd.merge( abcd, tot, how='inner', on='assortment' )\n",
    "abcd.fillna( 0, inplace=True )\n",
    "abcd.loc[len(abcd.index)] = abcd.sum()\n",
    "abcd.iloc[3, 0] = \"Total\"\n",
    "# Plotting\n",
    "plot_assortment( 'basic', df4 )\n",
    "plot_assortment( 'extended', df4 )\n",
    "plot_assortment( 'extra', df4 )\n",
    "# Plotting totals\n",
    "plot_assortment( '', df4 )\n",
    "# Table\n",
    "abcd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.4. Periodic Promotions - promo2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many stores take part in PROMO2 or not?\n",
    "# Answer: 571 stores are in PROMO2 and the other 544 are not.\n",
    "aux = df4.loc[:, ['promo2','store'] ].groupby('promo2').nunique().reset_index()\n",
    "aux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IS_PROMO2 & days of a week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plottings bellow indicate that:\n",
    "1. The best days to run a promotion are Wednewsday & Thrusday.\n",
    "2. On the other days, promotional price reduction results in less income."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROMO2: Promotions & its effect on sales during the days of a month\n",
    "sales0  = df4.loc[ (df4['promo2'] == 1)&(df4['is_promo2'] == 0), ['sales','day_of_week'] ].groupby('day_of_week').mean().reset_index()\n",
    "custom0 = df4.loc[ (df4['promo2'] == 1)&(df4['is_promo2'] == 0), ['customers','day_of_week'] ].groupby('day_of_week').mean().reset_index()\n",
    "sales1  = df4.loc[ (df4['promo2'] == 1)&(df4['is_promo2'] == 1), ['sales','day_of_week'] ].groupby('day_of_week').mean().reset_index()\n",
    "custom1 = df4.loc[ (df4['promo2'] == 1)&(df4['is_promo2'] == 1), ['customers','day_of_week'] ].groupby('day_of_week').mean().reset_index()\n",
    "sales1.columns = ['day_of_week','sales_promo']\n",
    "custom1.columns = ['day_of_week','customers_promo']\n",
    "\n",
    "res_day = pd.merge( sales0, sales1, how='inner', on='day_of_week' )\n",
    "res_day = pd.merge( res_day, custom0, how='inner', on='day_of_week' )\n",
    "res_day = pd.merge( res_day, custom1, how='inner', on='day_of_week' )\n",
    "\n",
    "res_day['sales_promo_dif'] = res_day['sales_promo'] - res_day['sales']\n",
    "res_day['customers_promo_dif'] = res_day['customers_promo'] - res_day['customers']\n",
    "dif = res_day.loc[:, ['day_of_week','sales_promo_dif','customers_promo_dif']]\n",
    "\n",
    "ax = res_day.plot( kind='bar', x='day_of_week', y=['sales','sales_promo'], figsize=(10,4), width=0.9 );\n",
    "ax.set_ylim(5000, 8000);\n",
    "ax = res_day.plot( kind='bar', x='day_of_week', y=['customers','customers_promo'], figsize=(10,4), width=0.9 );\n",
    "ax.set_ylim(550, 1220);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IS_PROMO2 & days of a month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plottings bellow indicate that:\n",
    "1. During one month of promotion (promo2), the increment in sales occur only from 6th to 15th day.\n",
    "2. During the days of strong sales, from 29th to 5th day and from 16th to 20th day, promotional price reduction results in less income.\n",
    "3. It's recomended to restrict promotions to the period from 6th to 15th day of each month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROMO2: Promotions & its effect on sales during the days of a month\n",
    "sales0  = df4.loc[ (df4['promo2'] == 1)&(df4['is_promo2'] == 0), ['sales','day'] ].groupby('day').mean().reset_index()\n",
    "custom0 = df4.loc[ (df4['promo2'] == 1)&(df4['is_promo2'] == 0), ['customers','day'] ].groupby('day').mean().reset_index()\n",
    "sales1  = df4.loc[ (df4['promo2'] == 1)&(df4['is_promo2'] == 1), ['sales','day'] ].groupby('day').mean().reset_index()\n",
    "custom1 = df4.loc[ (df4['promo2'] == 1)&(df4['is_promo2'] == 1), ['customers','day'] ].groupby('day').mean().reset_index()\n",
    "sales1.columns = ['day','sales_promo']\n",
    "custom1.columns = ['day','customers_promo']\n",
    "\n",
    "res_day = pd.merge( sales0, sales1, how='inner', on='day' )\n",
    "res_day = pd.merge( res_day, custom0, how='inner', on='day' )\n",
    "res_day = pd.merge( res_day, custom1, how='inner', on='day' )\n",
    "\n",
    "ax = res_day.plot( kind='bar', x='day', y=['sales','sales_promo'], figsize=(10,4), width=0.9 );\n",
    "ax.set_ylim(5000, 8300);\n",
    "ax = res_day.plot( kind='bar', x='day', y=['customers','customers_promo'], figsize=(10,4), width=0.9 );\n",
    "ax.set_ylim(550, 800);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IS_PROMO2 & months of the year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plottings bellow indicate that:\n",
    "1. The best months to run a promotion are January, April, July & October\n",
    "2. On the other months, promotional price reduction results in less income."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROMO2: Promotions & its effect on sales during the months of a year\n",
    "sales0  = df4.loc[ (df4['promo2'] == 1)&(df4['is_promo2'] == 0), ['sales','month'] ].groupby('month').mean().reset_index()\n",
    "custom0 = df4.loc[ (df4['promo2'] == 1)&(df4['is_promo2'] == 0), ['customers','month'] ].groupby('month').mean().reset_index()\n",
    "sales1  = df4.loc[ (df4['promo2'] == 1)&(df4['is_promo2'] == 1), ['sales','month'] ].groupby('month').mean().reset_index()\n",
    "custom1 = df4.loc[ (df4['promo2'] == 1)&(df4['is_promo2'] == 1), ['customers','month'] ].groupby('month').mean().reset_index()\n",
    "sales1.columns = ['month','sales_promo']\n",
    "custom1.columns = ['month','customers_promo']\n",
    "\n",
    "res_day = pd.merge( sales0, sales1, how='inner', on='month' )\n",
    "res_day = pd.merge( res_day, custom0, how='inner', on='month' )\n",
    "res_day = pd.merge( res_day, custom1, how='inner', on='month' )\n",
    "\n",
    "ax = res_day.plot( kind='bar', x='month', y=['sales','sales_promo'], figsize=(10,4), width=0.9 );\n",
    "ax.set_ylim(5500, 8300);\n",
    "ax = res_day.plot( kind='bar', x='month', y=['customers','customers_promo'], figsize=(10,4), width=0.9 );\n",
    "ax.set_ylim(600, 810);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.5. Eventual 1-day Promotions - promo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each store, one day may be promotional and the other not.\n",
    "# Number of regular days. . : 467.463 (55%)\n",
    "# Number of promotional days: 376.875 (45%) [ps: many promotions !!]\n",
    "promo_table = df4.loc[:, ['promo','date'] ].groupby('promo').count().reset_index()\n",
    "\n",
    "# PROMO & days of a week\n",
    "sales0  = df4.loc[ df4['promo'] == 0, ['sales','day_of_week'] ].groupby('day_of_week').mean().reset_index()\n",
    "sales1  = df4.loc[ df4['promo'] == 1, ['sales','day_of_week'] ].groupby('day_of_week').mean().reset_index()\n",
    "sales1.columns = ['day_of_week','sales_promo']\n",
    "res_day = pd.merge( sales0, sales1, how='inner', on='day_of_week' )\n",
    "res_day['sales_promo_dif'] = res_day['sales_promo'] - res_day['sales']\n",
    "ax = res_day.plot( kind='bar', x='day_of_week', y=['sales','sales_promo'], figsize=(10,4), width=0.9 );\n",
    "ax.set_ylim(4000, 10500);\n",
    "\n",
    "# PROMO & days of a month\n",
    "sales0  = df4.loc[ df4['promo'] == 0, ['sales','day'] ].groupby('day').mean().reset_index()\n",
    "sales1  = df4.loc[ df4['promo'] == 1, ['sales','day'] ].groupby('day').mean().reset_index()\n",
    "sales1.columns = ['day','sales_promo']\n",
    "res_day = pd.merge( sales0, sales1, how='inner', on='day' )\n",
    "res_day['sales_promo_dif'] = res_day['sales_promo'] - res_day['sales']\n",
    "ax = res_day.plot( kind='bar', x='day', y=['sales','sales_promo'], figsize=(10,4), width=0.9 );\n",
    "ax.set_ylim(4000, 10500);\n",
    "\n",
    "# PROMO & months of the year\n",
    "sales0  = df4.loc[ df4['promo'] == 0, ['sales','month'] ].groupby('month').mean().reset_index()\n",
    "sales1  = df4.loc[ df4['promo'] == 1, ['sales','month'] ].groupby('month').mean().reset_index()\n",
    "sales1.columns = ['month','sales_promo']\n",
    "res_day = pd.merge( sales0, sales1, how='inner', on='month' )\n",
    "res_day['sales_promo_dif'] = res_day['sales_promo'] - res_day['sales']\n",
    "ax = res_day.plot( kind='bar', x='month', y=['sales','sales_promo'], figsize=(10,4), width=0.9 );\n",
    "ax.set_ylim(4000, 10500);\n",
    "\n",
    "promo_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.6. Holidays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SCHOOL_HOLIDAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCHOOL HOLIDAYS & days of week\n",
    "sales0  = df4.loc[ df4['school_holiday'] == 0, ['sales','day_of_week'] ].groupby('day_of_week').mean().reset_index()\n",
    "custom0 = df4.loc[ df4['school_holiday'] == 0, ['customers','day_of_week'] ].groupby('day_of_week').mean().reset_index()\n",
    "sales1  = df4.loc[ df4['school_holiday'] == 1, ['sales','day_of_week'] ].groupby('day_of_week').mean().reset_index()\n",
    "custom1 = df4.loc[ df4['school_holiday'] == 1, ['customers','day_of_week'] ].groupby('day_of_week').mean().reset_index()\n",
    "sales1.columns  = ['day_of_week','sales_school_holiday']\n",
    "custom1.columns = ['day_of_week','customers_school_holiday']\n",
    "res = pd.merge( sales0, sales1, how='inner', on='day_of_week' )\n",
    "res = pd.merge( res, custom0, how='inner', on='day_of_week' )\n",
    "res = pd.merge( res, custom1, how='inner', on='day_of_week' )\n",
    "fig, axes = plt.subplots( nrows=2, ncols=1)\n",
    "ax1 = res.plot( ax=axes[0], kind='bar', x='day_of_week', y=['sales','sales_school_holiday'], figsize=(10,3), width=0.9 );\n",
    "ax1.set_ylim(5000, 12500);\n",
    "ax2 = res.plot( ax=axes[1], kind='bar', x='day_of_week', y=['customers','customers_school_holiday'], figsize=(10,3), width=0.9 );\n",
    "ax2.set_ylim(500, 2300);\n",
    "\n",
    "# SCHOOL HOLIDAYS & days of the month\n",
    "sales0  = df4.loc[ df4['school_holiday'] == 0, ['sales','day'] ].groupby('day').mean().reset_index()\n",
    "custom0 = df4.loc[ df4['school_holiday'] == 0, ['customers','day'] ].groupby('day').mean().reset_index()\n",
    "sales1  = df4.loc[ df4['school_holiday'] == 1, ['sales','day'] ].groupby('day').mean().reset_index()\n",
    "custom1 = df4.loc[ df4['school_holiday'] == 1, ['customers','day'] ].groupby('day').mean().reset_index()\n",
    "sales1.columns  = ['day','sales_school_holiday']\n",
    "custom1.columns = ['day','customers_school_holiday']\n",
    "res = pd.merge( sales0, sales1, how='inner', on='day' )\n",
    "res = pd.merge( res, custom0, how='inner', on='day' )\n",
    "res = pd.merge( res, custom1, how='inner', on='day' )\n",
    "fig, axes = plt.subplots( nrows=2, ncols=1)\n",
    "ax1 = res.plot( ax=axes[0], kind='bar', x='day', y=['sales','sales_school_holiday'], figsize=(10,5), width=0.9 );\n",
    "ax1.set_ylim(5000, 9500);\n",
    "ax2 = res.plot( ax=axes[1], kind='bar', x='day', y=['customers','customers_school_holiday'], figsize=(10,5), width=0.9 );\n",
    "ax2.set_ylim(600, 950);\n",
    "\n",
    "# SCHOOL HOLIDAYS & months of the year\n",
    "sales0  = df4.loc[ df4['school_holiday'] == 0, ['sales','month'] ].groupby('month').mean().reset_index()\n",
    "custom0 = df4.loc[ df4['school_holiday'] == 0, ['customers','month'] ].groupby('month').mean().reset_index()\n",
    "sales1  = df4.loc[ df4['school_holiday'] == 1, ['sales','month'] ].groupby('month').mean().reset_index()\n",
    "custom1 = df4.loc[ df4['school_holiday'] == 1, ['customers','month'] ].groupby('month').mean().reset_index()\n",
    "sales1.columns  = ['month','sales_school_holiday']\n",
    "custom1.columns = ['month','customers_school_holiday']\n",
    "res = pd.merge( sales0, sales1, how='inner', on='month' )\n",
    "res = pd.merge( res, custom0, how='inner', on='month' )\n",
    "res = pd.merge( res, custom1, how='inner', on='month' )\n",
    "fig, axes = plt.subplots( nrows=2, ncols=1)\n",
    "ax1 = res.plot( ax=axes[0], kind='bar', x='month', y=['sales','sales_school_holiday'], figsize=(10,4), width=0.9 );\n",
    "ax1.set_ylim(5000, 9500);\n",
    "ax2 = res.plot( ax=axes[1], kind='bar', x='month', y=['customers','customers_school_holiday'], figsize=(10,4), width=0.9 );\n",
    "ax2.set_ylim(600, 1000);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STATE_HOLIDAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Days of holiday\n",
    "days_of_holiday  = df4.loc[ df4['state_holiday'] != 'regular_day', ['sales','month','day'] ].groupby(['month','day']).count().reset_index()\n",
    "days_of_holiday.columns = ['month','day','occurrence']\n",
    "days_of_holiday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STATE HOLIDAYS & days of week\n",
    "sales0  = df4.loc[ df4['state_holiday'] == 'regular_day', ['sales','day_of_week'] ].groupby('day_of_week').mean().reset_index()\n",
    "custom0 = df4.loc[ df4['state_holiday'] == 'regular_day', ['customers','day_of_week'] ].groupby('day_of_week').mean().reset_index()\n",
    "sales1  = df4.loc[ df4['state_holiday'] != 'regular_day', ['sales','day_of_week'] ].groupby('day_of_week').mean().reset_index()\n",
    "custom1 = df4.loc[ df4['state_holiday'] != 'regular_day', ['customers','day_of_week'] ].groupby('day_of_week').mean().reset_index()\n",
    "sales1.columns  = ['day_of_week','sales_state_holiday']\n",
    "custom1.columns = ['day_of_week','customers_state_holiday']\n",
    "res = pd.merge( sales0, sales1, how='inner', on='day_of_week' )\n",
    "res = pd.merge( res, custom0, how='inner', on='day_of_week' )\n",
    "res = pd.merge( res, custom1, how='inner', on='day_of_week' )\n",
    "fig, axes = plt.subplots( nrows=2, ncols=1)\n",
    "ax1 = res.plot( ax=axes[0], kind='bar', x='day_of_week', y=['sales','sales_state_holiday'], figsize=(10,3), width=0.9 );\n",
    "#ax1.set_ylim(4000, 16000);\n",
    "ax2 = res.plot( ax=axes[1], kind='bar', x='day_of_week', y=['customers','customers_state_holiday'], figsize=(10,3), width=0.9 );\n",
    "#ax2.set_ylim(300, 3000);\n",
    "\n",
    "# STATE HOLIDAYS & days of the month\n",
    "sales0  = df4.loc[ df4['state_holiday'] == 'regular_day', ['sales','day'] ].groupby('day').mean().reset_index()\n",
    "custom0 = df4.loc[ df4['state_holiday'] == 'regular_day', ['customers','day'] ].groupby('day').mean().reset_index()\n",
    "sales1  = df4.loc[ df4['state_holiday'] != 'regular_day', ['sales','day'] ].groupby('day').mean().reset_index()\n",
    "days_of_holiday = sales1\n",
    "custom1 = df4.loc[ df4['state_holiday'] != 'regular_day', ['customers','day'] ].groupby('day').mean().reset_index()\n",
    "sales1.columns  = ['day','sales_state_holiday']\n",
    "custom1.columns = ['day','customers_state_holiday']\n",
    "res = pd.merge( sales0, sales1, how='inner', on='day' )\n",
    "res = pd.merge( res, custom0, how='inner', on='day' )\n",
    "res = pd.merge( res, custom1, how='inner', on='day' )\n",
    "fig, axes = plt.subplots( nrows=2, ncols=1)\n",
    "ax1 = res.plot( ax=axes[0], kind='bar', x='day', y=['sales','sales_state_holiday'], figsize=(10,5), width=0.9 );\n",
    "ax1.set_ylim(4000, 13200);\n",
    "ax2 = res.plot( ax=axes[1], kind='bar', x='day', y=['customers','customers_state_holiday'], figsize=(10,5), width=0.9 );\n",
    "ax2.set_ylim(500, 2500);\n",
    "\n",
    "# STATE HOLIDAYS & days of week\n",
    "sales0  = df4.loc[ df4['state_holiday'] == 'regular_day', ['sales','month'] ].groupby('month').mean().reset_index()\n",
    "custom0 = df4.loc[ df4['state_holiday'] == 'regular_day', ['customers','month'] ].groupby('month').mean().reset_index()\n",
    "sales1  = df4.loc[ df4['state_holiday'] != 'regular_day', ['sales','month'] ].groupby('month').mean().reset_index()\n",
    "custom1 = df4.loc[ df4['state_holiday'] != 'regular_day', ['customers','month'] ].groupby('month').mean().reset_index()\n",
    "sales1.columns  = ['month','sales_state_holiday']\n",
    "custom1.columns = ['month','customers_state_holiday']\n",
    "res = pd.merge( sales0, sales1, how='inner', on='month' )\n",
    "res = pd.merge( res, custom0, how='inner', on='month' )\n",
    "res = pd.merge( res, custom1, how='inner', on='month' )\n",
    "fig, axes = plt.subplots( nrows=2, ncols=1)\n",
    "ax1 = res.plot( ax=axes[0], kind='bar', x='month', y=['sales','sales_state_holiday'], figsize=(10,4), width=0.9 );\n",
    "ax1.set_ylim(4000, 14000);\n",
    "ax2 = res.plot( ax=axes[1], kind='bar', x='month', y=['customers','customers_state_holiday'], figsize=(10,4), width=0.9 );\n",
    "ax2.set_ylim(400, 2600);\n",
    "\n",
    "# table of quantities:\n",
    "total = df4.shape[0]\n",
    "aux11 = df4.loc[:, ['state_holiday', 'store']].copy()\n",
    "stt_hol = aux11.groupby('state_holiday').count().reset_index()\n",
    "stt_hol['perc'] = ( 100. * stt_hol['store'] / total ).round(2)\n",
    "stt_hol.columns = [ 'state_holiday', 'qty', 'qty%' ]\n",
    "stt_hol.sort_values('qty', ascending=False)\n",
    "\n",
    "# Days of holiday\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.7. Competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPETITION & days of a week\n",
    "sales0  = df4.loc[ df4['has_competition'] == 0, ['sales','day_of_week'] ].groupby('day_of_week').mean().reset_index()\n",
    "sales1  = df4.loc[ df4['has_competition'] == 1, ['sales','day_of_week'] ].groupby('day_of_week').mean().reset_index()\n",
    "sales1.columns = ['day_of_week','sales_competition']\n",
    "res_day = pd.merge( sales0, sales1, how='inner', on='day_of_week' )\n",
    "ax = res_day.plot( kind='bar', x='day_of_week', y=['sales','sales_competition'], figsize=(10,4), width=0.9 );\n",
    "ax.set_ylim(5500, 9500);\n",
    "\n",
    "# COMPETITION & days of a month\n",
    "sales0  = df4.loc[ df4['has_competition'] == 0, ['sales','day'] ].groupby('day').mean().reset_index()\n",
    "sales1  = df4.loc[ df4['has_competition'] == 1, ['sales','day'] ].groupby('day').mean().reset_index()\n",
    "sales1.columns = ['day','sales_competition']\n",
    "res_day = pd.merge( sales0, sales1, how='inner', on='day' )\n",
    "ax = res_day.plot( kind='bar', x='day', y=['sales','sales_competition'], figsize=(10,4), width=0.9 );\n",
    "ax.set_ylim(5500, 9500);\n",
    "\n",
    "# COMPETITION & months of the year\n",
    "sales0  = df4.loc[ df4['has_competition'] == 0, ['sales','month'] ].groupby('month').mean().reset_index()\n",
    "sales1  = df4.loc[ df4['has_competition'] == 1, ['sales','month'] ].groupby('month').mean().reset_index()\n",
    "sales1.columns = ['month','sales_competition']\n",
    "res_day = pd.merge( sales0, sales1, how='inner', on='month' )\n",
    "ax = res_day.plot( kind='bar', x='month', y=['sales','sales_competition'], figsize=(10,4), width=0.9 );\n",
    "ax.set_ylim(5500, 9500);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.8. Testing Hypotesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H1. Stores with long time competitors should sell more\n",
    "**FALSE hypothesis** Stores with long time competitors SELL LESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting avg(SALES) x COMPETITION_SINCE\n",
    "# Result: more recent COMPETITION-starting-point implies higher sales - H1 false.\n",
    "aux1 = df4[['competition_since', 'sales']].groupby( 'competition_since' ).mean().reset_index()\n",
    "aux1['timeline'] = aux1['competition_since'].astype( np.int64 )\n",
    "aux2 = aux1[ aux1['timeline']>0 ].copy()\n",
    "plt.figure( figsize=(10, 4) )\n",
    "sns.regplot( x='timeline', y='sales', data=aux2, scatter_kws={'color': 'black', 'alpha': 0.6}, line_kws={'color': 'red'}, ci=99 );\n",
    "plt.annotate( 'long time competitors', xy=(60,45), fontsize=9, xycoords='figure points' )\n",
    "plt.annotate( 'recent competitors', xy=(520,175), fontsize=9, xycoords='figure points' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H2. Stores close to other competitors should sell less\n",
    "**INCONCLUSIVE**: in average, competition distance does not affect sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis-2: Stores close to other competitors should sell less\n",
    "# Answer: the graphics indicates FALSE hypothesis.\n",
    "\n",
    "aux1 = df4.loc[ df4['has_competition'] == 1 , ['competition_distance', 'sales']].groupby( 'competition_distance' ).median().reset_index()\n",
    "bins = list( np.arange( 0, 20000, 1000 ) )\n",
    "aux1['competition_distance_binned'] = pd.cut( aux1['competition_distance'], bins=bins )\n",
    "aux2 = aux1[['competition_distance_binned', 'sales']].groupby( 'competition_distance_binned' ).median().reset_index()\n",
    "\n",
    "plt.figure( figsize=(10, 3) )\n",
    "plt.subplot( 1, 2, 1 )\n",
    "ax = sns.barplot( x='competition_distance_binned', y='sales', data=aux2 );\n",
    "ax.set_ylim(4000, 7000);\n",
    "plt.xticks( rotation=90 );\n",
    "\n",
    "plt.subplot( 1, 2, 2 )\n",
    "aux1 = df4[['competition_distance', 'sales']].groupby( 'competition_distance' ).mean().reset_index()\n",
    "sns.heatmap( aux1.corr( method='pearson' ), annot=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H3. Stores with low budget products (like \"extra\" assortment) sell less than stores with more expensive products (like \"extended\" assortment).\n",
    "**FALSE hypothesis**: Stores with low budget products SELL MORE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis-3: Stores with low budget products (like \"extra\" assortment) sell less...\n",
    "#               ...than stores with more expensive products (like \"extended\" assortment).\n",
    "# Answer: the graphics indicates FALSE hypothesis.\n",
    "\n",
    "plt.figure(figsize=(10,3))\n",
    "# Sales vs Assortment\n",
    "plt.subplot(1,3,2)\n",
    "b1 = sns.barplot(df4, x='assortment', y='sales');\n",
    "for i in b1.containers:\n",
    "    b1.bar_label(i,)\n",
    "plt.title('Daily Sales - All Stores');\n",
    "plt.xlabel(\"ASSORTMENT\");\n",
    "plt.ylabel(\"SALES\");\n",
    "plt.annotate( '...sells more!', xy=(0.75, 0.97), xycoords='axes fraction', \n",
    "              xytext=(0.20, 0.94), textcoords='axes fraction', \n",
    "              arrowprops=dict(facecolor='red', shrink=0.05),\n",
    "              horizontalalignment='center', verticalalignment='top' );\n",
    "# Customers per Assortment\n",
    "plt.subplot(1,3,3)\n",
    "b2 = sns.barplot(df4, x='assortment', y='customers');\n",
    "for i in b2.containers:\n",
    "    b2.bar_label(i,)\n",
    "plt.title('N# of Daily Customers');\n",
    "plt.xlabel(\"ASSORTMENT\");\n",
    "plt.ylabel(\" \");\n",
    "plt.annotate( '...to more people.', xy=(0.73, 0.96), xycoords='axes fraction', \n",
    "              xytext=(0.27, 0.85), textcoords='axes fraction', \n",
    "              arrowprops=dict(facecolor='red', shrink=0.05),\n",
    "              horizontalalignment='center', verticalalignment='top' );\n",
    "# Sales/Customer per Assortment\n",
    "plt.subplot(1,3,1)\n",
    "b3 = sns.barplot(df4, x='assortment', y='sales_per_customer');\n",
    "for i in b3.containers:\n",
    "    b3.bar_label(i,)\n",
    "plt.title('Daily Value per Customer');\n",
    "plt.xlabel(\"ASSORTMENT\");\n",
    "plt.ylabel(\" \");\n",
    "plt.annotate( 'Low budget...', xy=(0.83, 0.45), xycoords='axes fraction', \n",
    "              xytext=(0.83, 0.70), textcoords='axes fraction',\n",
    "              arrowprops=dict(facecolor='red', shrink=0.05),\n",
    "              horizontalalignment='center', verticalalignment='top' );\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H4. Stores with low budget products have more customers and stores with more expensive products have less customers.\n",
    "**TRUE hypothesis**: low budget products (like \"extra\" assortment) attract 2.75x more customers than the others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,2))\n",
    "# Customers per Assortment\n",
    "b2 = sns.barplot(df4, y='assortment', x='customers', orient='h');\n",
    "for i in b2.containers:\n",
    "    b2.bar_label(i, label_type='center')\n",
    "plt.title('N# of Daily Customers');\n",
    "plt.xlabel(\"N# of Customers\");\n",
    "plt.ylabel(\"Assortment\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H5. Most of the stores work with low budget products (like \"extra\" assortment).\n",
    "**FALSE hypothesis**: Out of 1115, only 9 stores (**0,81%**) work with \"extra\"assortment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assortment = df4.loc[:, ['assortment', 'store']].groupby('assortment').nunique().reset_index()\n",
    "assortment.columns = ['assortment', 'Qtty']\n",
    "assortment.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H6. Stores with longer periods of promotional sales should sell more\n",
    "**FALSE hypothesis** In average, stores with longer periods of promotional sales SELL LESS.\n",
    "- Besides that, it can be observed that:\n",
    "1. During one month of promotion (promo2), the increment in sales occur only from 6th to 15th day.\n",
    "2. During the days of strong sales, from 29th to 5th day and from 16th to 20th day, promotional price reduction results in less income.\n",
    "3. The best months to run a promotion are January, April, July & October\n",
    "4. On the other months, promotional price reduction results in less income.\n",
    "5. It's recomended to restrict promotions to periods of lower demand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting avg(SALES) x PROMO2_SINCE\n",
    "# Result: more recent PROMO2-starting-point implies higher sales - H4 false.\n",
    "aux1 = df4[['promo2_since', 'sales']].groupby( 'promo2_since' ).mean().reset_index()\n",
    "aux1['timeline'] = aux1['promo2_since'].astype( np.int64 )\n",
    "plt.figure( figsize=(10, 4) )\n",
    "sns.regplot( x='timeline', y='sales', data=aux1, scatter_kws={'color': 'black', 'alpha': 0.6}, line_kws={'color': 'red'}, ci=99 );\n",
    "plt.annotate( 'long time promotional sales', xy=(60,45), fontsize=9, xycoords='figure points' );\n",
    "plt.annotate( 'recent promotional sales', xy=(465,175), fontsize=9, xycoords='figure points' );\n",
    "\n",
    "# PROMO2: Promotions & its effect on sales during the days of a month\n",
    "sales0  = df4.loc[ (df4['promo2'] == 1)&(df4['is_promo2'] == 0), ['sales','day'] ].groupby('day').mean().reset_index()\n",
    "sales1  = df4.loc[ (df4['promo2'] == 1)&(df4['is_promo2'] == 1), ['sales','day'] ].groupby('day').mean().reset_index()\n",
    "sales1.columns = ['day','sales_promo']\n",
    "res_day = pd.merge( sales0, sales1, how='inner', on='day' )\n",
    "ax = res_day.plot( kind='bar', x='day', y=['sales','sales_promo'], figsize=(10,3), width=0.9 );\n",
    "ax.set_ylim(5000, 8300);\n",
    "\n",
    "# PROMO2: Promotions & its effect on sales during the months of a year\n",
    "sales0  = df4.loc[ (df4['promo2'] == 1)&(df4['is_promo2'] == 0), ['sales','month'] ].groupby('month').mean().reset_index()\n",
    "sales1  = df4.loc[ (df4['promo2'] == 1)&(df4['is_promo2'] == 1), ['sales','month'] ].groupby('month').mean().reset_index()\n",
    "sales1.columns = ['month','sales_promo']\n",
    "res_day = pd.merge( sales0, sales1, how='inner', on='month' )\n",
    "ax = res_day.plot( kind='bar', x='month', y=['sales','sales_promo'], figsize=(10,3), width=0.9 );\n",
    "ax.set_ylim(5500, 8300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H7. Stores with more consecutive promotional sales should sell more\n",
    "**FALSE hypothesis** Stores with more consecutive promotional sales sell LESS, in average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis-7: Stores with more consecutive promotional sales should sell more.\n",
    "# Stores which took part in PROMO2 finished selling LESS, in average.\n",
    "df4[['promo', 'promo2', 'sales' ]].groupby( ['promo', 'promo2'] ).mean().sort_values('sales', ascending=True).reset_index().round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H8. Stores open during Christmas Holiday should sell more\n",
    "**TRUE hypotesis**: In average, during Christmas Holiday, stores sell more than in regular days, even in December."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis-8: Stores selling during Christmas should sell more.\n",
    "# Answer: FALSE\n",
    "plt.figure( figsize=(10, 4) )\n",
    "plt.subplot( 1, 2, 1 )\n",
    "aux1 = df4[['state_holiday', 'sales']].groupby( 'state_holiday' ).mean().reset_index()\n",
    "sns.barplot( x='state_holiday', y='sales', data=aux1 );\n",
    "plt.xticks(rotation=5, fontsize=10);\n",
    "plt.subplot( 1, 2, 2 )\n",
    "aux2 = df4[['year', 'state_holiday', 'sales']].groupby( ['year', 'state_holiday'] ).mean().reset_index()\n",
    "ax = sns.barplot( x='year', y='sales', hue='state_holiday', data=aux2 );\n",
    "ax.set_ylim(0, 16000);\n",
    "# ...even in December:\n",
    "aux1 = df4.loc[ (df4['month']==12) & ( (df4['state_holiday']=='christmas') | (df4['state_holiday']=='regular_day') ), \n",
    "               ['month', 'state_holiday', 'sales']].copy()\n",
    "aux2 = aux1[['month', 'state_holiday', 'sales']].groupby( ['month', 'state_holiday'] ).mean().reset_index()\n",
    "aux2.columns = ['month','state_holiday','sales_in_december']\n",
    "aux2.round(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H9. Stores should sell less during school holidays, since there is a reduced number of customers.\n",
    "**FALSE hypothesis**: Stores SELL MORE during school holidays, except in December, when they do sell less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis-9: Stores should sell less during school holidays.\n",
    "# Answer: FALSE.\n",
    "\n",
    "# oops: 'numpy.int64' object has no attribute 'startswith'\n",
    "# python -m pip install svgutils==0.3.1\n",
    "# https://neurostars.org/t/attributeerror-numpy-int64-object-has-no-attribute-value/17969\n",
    "\n",
    "plt.figure( figsize=(10, 6) )\n",
    "\n",
    "aux2 = df4[['month', 'school_holiday', 'sales']].groupby(['month', 'school_holiday']).mean().reset_index()\n",
    "plt.subplot( 3, 1, 1 )\n",
    "ax = sns.barplot( x='month', y='sales', hue='school_holiday', data=aux2 );\n",
    "ax.set_ylim(5000, 9000);\n",
    "\n",
    "aux2 = df4[['month', 'school_holiday', 'customers']].groupby(['month', 'school_holiday']).mean().reset_index()\n",
    "plt.subplot( 3, 1, 2 )\n",
    "ax = sns.barplot( x='month', y='customers', hue='school_holiday', data=aux2 );\n",
    "ax.set_ylim(500, 1100);\n",
    "\n",
    "aux1 = df4[['school_holiday', 'sales']].groupby('school_holiday').mean().reset_index()\n",
    "plt.subplot( 3, 1, 3 )\n",
    "sns.barplot( y='school_holiday', x='sales', orient='h', data=aux1 );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H10. Stores should also sell less during other holidays.\n",
    "**FALSE hypothesis**: Stores open in holidays SELL MORE then in regular days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis-8: Stores selling during Christmas should sell more.\n",
    "# Answer: FALSE\n",
    "plt.figure( figsize=(10, 4) )\n",
    "plt.subplot( 1, 2, 1 )\n",
    "aux1 = df4[['state_holiday', 'sales']].groupby( 'state_holiday' ).mean().reset_index()\n",
    "sns.barplot( x='state_holiday', y='sales', data=aux1 );\n",
    "plt.xticks(rotation=5, fontsize=10);\n",
    "plt.subplot( 1, 2, 2 )\n",
    "aux2 = df4[['year', 'state_holiday', 'sales']].groupby( ['year', 'state_holiday'] ).mean().reset_index()\n",
    "ax = sns.barplot( x='year', y='sales', hue='state_holiday', data=aux2 );\n",
    "ax.set_ylim(0, 16000);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H11. Stores should sell more on Mondays, since most of the stores were closed the day before.\n",
    "**TRUE hypothesis**: In median, stores SELL MORE on Mondays than on the other days. On Monday, it can also be observed:\n",
    "- higher total sales.\n",
    "- higher number of sales per customer.\n",
    "- high number of customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "week_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H12. On Sunday, the few stores open (33/1115) should sell more.\n",
    "**TRUE hypothesis**: In average, stores SELL MORE on Sundays. It can also be observed that:\n",
    "- Stores receive a duplicated amount of consumers.\n",
    "- Each consumer buy cheaper products (costing $6,19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "week_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H13. On Saturday, the stores should sell less, due to the reduced working hours.\n",
    "**TRUE hypothesis**: In average, Saturdays have the weakest sales of the week.\n",
    "- PS: the results may be due to the reduced working hours, from 9 a.m. to 1 p.m., according to \"https://www.bonn.de/microsite/en/services/medical-care/pharmacies.php#:~:text=A%20pharmacy%20in%20Germany%20is,to%201%20p.m.%20on%20Saturdays.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "week_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H14. Due to Mother's Day, May should present the second best selling month of the year.\n",
    "**FALSE hypothesis**: In average, May is the third best month.\n",
    "- Plotting indicates:\n",
    "1. DECEMBER: Best seller month. Revenue impacted by Christmas.\n",
    "2. NOVEMBER: Second stronger month, possibly caused by black friday sales.\n",
    "3. MAY: Third stronger month, probably caused by Mother's Day.\n",
    "4. SEPTEMBER, JANUARY & FEBRUARY: these months show the lowest selling statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis-14: Due to Mother's Day, May should present the...\n",
    "#               ...second best selling month of the year.\n",
    "# Answer: TRUE\n",
    "Avg_Sales_Month = df4.loc[:, ['month','sales']].groupby('month').mean().reset_index()\n",
    "plt.figure( figsize=(10, 4) )\n",
    "plt.subplot( 1, 2, 1 )\n",
    "ax = sns.barplot( x='month', y='sales', data=Avg_Sales_Month );\n",
    "ax.set_ylim(6200, 8700);\n",
    "plt.annotate( '1st. December (Christmas)', xy=(0.90, 0.96), xycoords='axes fraction', \n",
    "              xytext=(0.26, 0.96), textcoords='axes fraction', \n",
    "              arrowprops=dict(facecolor='red', shrink=0.05),\n",
    "              horizontalalignment='center', verticalalignment='top' );\n",
    "plt.annotate( '2nd. November (black friday)', xy=(0.85, 0.40), xycoords='axes fraction', \n",
    "              xytext=(0.28, 0.83), textcoords='axes fraction', \n",
    "              arrowprops=dict(facecolor='red', shrink=0.05),\n",
    "              horizontalalignment='center', verticalalignment='top' );\n",
    "plt.annotate( '3rd. May (Mothers\\' Day)', xy=(0.38, 0.35), xycoords='axes fraction', \n",
    "              xytext=(0.23, 0.70), textcoords='axes fraction', \n",
    "              arrowprops=dict(facecolor='red', shrink=0.05),\n",
    "              horizontalalignment='center', verticalalignment='top' );\n",
    "plt.subplot( 1, 2, 2 )\n",
    "sns.regplot( x='month', y='sales', data=Avg_Sales_Month );\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H15. Due to the winter, January and February should sell less.\n",
    "**TRUE hypothesis**: January and February, along with September, are the worst selling months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis-15: Due to the winter, January and February should sell less.\n",
    "# Answer: TRUE\n",
    "Avg_Sales_Month = df4.loc[:, ['month','sales']].groupby('month').mean().reset_index()\n",
    "plt.figure( figsize=(10, 4) )\n",
    "plt.subplot( 1, 2, 1 )\n",
    "ax = sns.barplot( x='month', y='sales', data=Avg_Sales_Month );\n",
    "ax.set_ylim(6200, 8700);\n",
    "plt.annotate( 'worst selling months', xy=(0.20, 0.45), xycoords='axes fraction' )\n",
    "plt.annotate( ' ', xy=(0.13, 0.14), xycoords='axes fraction', \n",
    "              xytext=(0.13, 0.44), textcoords='axes fraction', \n",
    "              arrowprops=dict(facecolor='red', shrink=0.05),\n",
    "              horizontalalignment='center', verticalalignment='top' );\n",
    "plt.annotate( ' ', xy=(0.71, 0.14), xycoords='axes fraction', \n",
    "              xytext=(0.71, 0.44), textcoords='axes fraction', \n",
    "              arrowprops=dict(facecolor='red', shrink=0.05),\n",
    "              horizontalalignment='center', verticalalignment='top' );\n",
    "plt.annotate( ' ', xy=(0.04, 0.14), xycoords='axes fraction', \n",
    "              xytext=(0.04, 0.44), textcoords='axes fraction', \n",
    "              arrowprops=dict(facecolor='red', shrink=0.05),\n",
    "              horizontalalignment='center', verticalalignment='top' );\n",
    "plt.subplot( 1, 2, 2 )\n",
    "sns.regplot( x='month', y='sales', data=Avg_Sales_Month );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H16. Stores should sell more from the last days of the previous month to the first days of the current month, due to the effect of salaries in the economy.\n",
    "**TRUE hypothesis**: Stores SELL MORE from the last days of the previous month to the first days of the current month.\n",
    "- The strongest peak of sales occur from the 29th up to the 4th or 5th of the following month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis-16: \n",
    "# Answer: TRUE\n",
    "Avg_Sales_Month = df4.loc[:, ['day','sales']].groupby('day').mean().reset_index()\n",
    "plt.figure( figsize=(10, 4) )\n",
    "plt.subplot( 1, 2, 1 )\n",
    "ax = sns.barplot( x='day', y='sales', data=Avg_Sales_Month );\n",
    "ax.set_ylim(5500, 8500);\n",
    "plt.annotate( ' ', xy=(0.83, 0.80), xycoords='axes fraction', \n",
    "              xytext=(0.63, 0.95), textcoords='axes fraction', \n",
    "              arrowprops=dict(facecolor='red', shrink=0.05),\n",
    "              horizontalalignment='center', verticalalignment='top' );\n",
    "plt.annotate( ' ', xy=(0.20, 0.80), xycoords='axes fraction', \n",
    "              xytext=(0.40, 0.95), textcoords='axes fraction', \n",
    "              arrowprops=dict(facecolor='red', shrink=0.05),\n",
    "              horizontalalignment='center', verticalalignment='top' );\n",
    "plt.subplot( 1, 2, 2 )\n",
    "sns.regplot( x='day', y='sales', data=Avg_Sales_Month );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H17. Stores should sell less in the middle days of the month.\n",
    "**FALSE hypothesis**: Another peak occurs in the 3rd week, between the 16th and 19th\n",
    "- There is a clear seasonal behaviour, showing two periods of peak and two periods of valey.\n",
    "- The weakest periods occur between the 10th and 13th, and again in two weeks, between the 24th and 26th."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis-17: \n",
    "# Answer: FALSE\n",
    "Avg_Sales_Month = df4.loc[:, ['day','sales']].groupby('day').mean().reset_index()\n",
    "plt.figure( figsize=(10, 4) )\n",
    "plt.subplot( 1, 2, 1 )\n",
    "ax = sns.barplot( x='day', y='sales', data=Avg_Sales_Month );\n",
    "ax.set_ylim(5500, 8500);\n",
    "plt.annotate( ' ', xy=(0.53, 0.66), xycoords='axes fraction', \n",
    "              xytext=(0.53, 0.91), textcoords='axes fraction', \n",
    "              arrowprops=dict(facecolor='red', shrink=0.05),\n",
    "              horizontalalignment='center', verticalalignment='top' );\n",
    "plt.subplot( 1, 2, 2 )\n",
    "sns.regplot( x='day', y='sales', data=Avg_Sales_Month );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.9. Summary of Hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2.1. RESUMO DAS HIPÓTESES\n",
    "# Tabulação dos resultados\n",
    "tab = [\n",
    "    ['Hipóteses', 'Conclusão', 'Relevância', 'Feature'],\n",
    "    ['H1',  'Falsa', 'Média', 'competition_since'],\n",
    "    ['H2',  '-', 'Baixa', 'competition_distance'],\n",
    "    ['H3',  'Falsa', 'Alta', 'assortment'],\n",
    "    ['H4',  'Verdadeira', 'Média', 'assortment'],\n",
    "    ['H5',  'Falsa', 'Média', 'assortment'],\n",
    "    ['H6',  'Falsa', 'Média', 'promo2_since'],\n",
    "    ['H7',  'Falsa', 'Média', 'promo/promo2'],\n",
    "    ['H8',  'Verdadeira', 'Alta', 'state_holiday/christmas'],\n",
    "    ['H9', 'Falsa', 'Alta', 'school_holiday'],\n",
    "    ['H10', 'Falsa', 'Alta', 'state_holiday'],\n",
    "    ['H11', 'Verdadeira', 'Baixa', 'day_of_week'],\n",
    "    ['H12', 'Verdadeira', 'Média', 'day_of_week'],\n",
    "    ['H13', 'Verdadeira', 'Alta', 'day_of_week'],\n",
    "    ['H14', 'Falsa', 'Alta', 'month/state_holiday'],\n",
    "    ['H15', 'Verdadeira', 'Alta', 'month'],\n",
    "    ['H16', 'Verdadeira', 'Alta', 'day'],\n",
    "    ['H17', 'Falsa', 'Alta', 'day']\n",
    "]\n",
    "print( tabulate( tab, headers='firstrow' ) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Multivariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aim: discover relation among attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DELETE 'CUSTOMERS' & 'SALES_PER_CUSTOMER' COLUMNS:\n",
    "# - According to section 3.2\n",
    "# - Del 'customers' since it won't be available during prediction (=business restriction)\n",
    "# - Del 'sales_per_customer', since 'customers' was deleted.\n",
    "cols_drop = [ 'customers', 'sales_per_customer' ]\n",
    "df4 = df4.drop( cols_drop, axis=1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1. Numerical Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3.1. MULTIVARIATE ANALYSIS - Numerical Attributes\n",
    "# Select numerical attributes for correlation\n",
    "# PS: excluding \"CUSTOMERS\" & \"SALES PER CUSTOMER\", due to business restriction.\n",
    "num_attributes = num_attributes.loc[:,\n",
    "                   ['sales',\n",
    "                    'competition_distance',\n",
    "                    'promo2_time_week',\n",
    "                    'competition_time_month']]\n",
    "\n",
    "num_vars = len(num_attributes.columns)\n",
    "correlation_matrix = np.zeros((num_vars, num_vars))\n",
    "\n",
    "correlation = num_attributes.corr( method='pearson' )\n",
    "plt.figure( figsize=(8, 4) );\n",
    "sns.heatmap( correlation, annot=True );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2. Categorical Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3.1. MULTIVARIATE ANALYSIS - Categorical Attributes\n",
    "# Assembling 'correlation' matrix for categorical attributes\n",
    "\n",
    "# Select categorical features to apply CRAMER'S V function\n",
    "df_cramer = df4.loc[:,\n",
    "       ['store',              'sales',              'date',              'school_holiday',\n",
    "        'state_holiday',      'year',               'semester',          'quarter',\n",
    "        '2months',            'month',              'fortnight_of_year', 'fortnight_of_month',\n",
    "        'day',                'day_of_week',        'week_of_year',      'year_week',\n",
    "        'promo',              'promo2',             'is_promo2',         'promo2_since',\n",
    "        'promo2_since_week',  'promo2_since_year',  'store_type',        'assortment',\n",
    "        'has_competition',    'competition_open_since_month',            'competition_open_since_year',\n",
    "        'competition_since' ]]\n",
    "\n",
    "num_vars = len(df_cramer.columns)\n",
    "correlation_matrix = np.zeros((num_vars, num_vars))\n",
    "\n",
    "# Create correlation matrix using CRAMER'S V function\n",
    "for i in range(num_vars):\n",
    "    for j in range(num_vars):\n",
    "        if i != j:\n",
    "            corr = cramer_v(df_cramer.iloc[:, i], df_cramer.iloc[:, j])\n",
    "            correlation_matrix[i, j] = corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "correlation_df = pd.DataFrame(correlation_matrix, columns=df_cramer.columns, index=df_cramer.columns)\n",
    "sns.set(font_scale=0.6)\n",
    "sns.heatmap(correlation_df.corr(), annot=True, fmt='.2f', cmap='BrBG');\n",
    "plt.title(\"Categoric Features Correlation (cramer V)\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 5. DATA PREPARATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.0. Initial Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = df4.copy()\n",
    "\n",
    "rs = RobustScaler()\n",
    "mms = MinMaxScaler()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Normalizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.1 Response Variable Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1.1 Response Variable Normalization\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(2,3,1)\n",
    "sns.distplot( df5['sales'] );\n",
    "plt.xticks( fontsize=7 );\n",
    "plt.yticks( rotation=60, fontsize=7 );\n",
    "plt.annotate( 'Original', xy=(0.45, 0.9), xycoords='axes fraction' )\n",
    "#\n",
    "plt.subplot(2,3,2)\n",
    "sns.distplot( np.sqrt(df5['sales']) );\n",
    "plt.xticks( fontsize=7 );\n",
    "plt.yticks( fontsize=7 );\n",
    "plt.annotate( 'Sqrt()', xy=(0.45, 0.9), xycoords='axes fraction' )\n",
    "#\n",
    "plt.subplot(2,3,3)\n",
    "sns.distplot( np.log1p(df5['sales']) );\n",
    "plt.xticks( fontsize=7 );\n",
    "plt.yticks( fontsize=7 );\n",
    "plt.annotate( 'Log()', xy=(0.45, 0.9), xycoords='axes fraction' )\n",
    "plt.title('SALES')\n",
    "# Boxplot\n",
    "plt.subplot(2,3,4)\n",
    "bp = sns.boxplot( x='sales', data=df5 );\n",
    "#\n",
    "plt.subplot(2,3,5)\n",
    "bp = sns.boxplot( x=np.sqrt(df5['sales']) );\n",
    "\n",
    "# THE TRANSFORMATION !!\n",
    "df5['sales'] = np.log1p( df5['sales'] )\n",
    "\n",
    "plt.subplot(2,3,6)\n",
    "bp = sns.boxplot( x='sales', data=df5 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Rescaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical variables (see 4.1.1): promo2_time_week, Competition Distance & competition_time_month\n",
    "# Géron, pg.76: \"...before you scale the feature, you should first transform it to shrink the heavy tail\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.1 Rescaling: promo2_time_week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 Rescaling: promo2_time_week\n",
    "# Just apply min-max scaling\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(2,2,1)\n",
    "sns.distplot( df5['promo2_time_week'] );\n",
    "plt.xticks( fontsize=7 );\n",
    "plt.yticks( rotation=60, fontsize=7 );\n",
    "plt.annotate( 'What if we disregard points at zero?', xy=(0.13, 0.45), xycoords='axes fraction', \n",
    "              xytext=(0.43, 0.95), textcoords='axes fraction',\n",
    "              arrowprops=dict(facecolor='red', shrink=0.05),\n",
    "              horizontalalignment='center', verticalalignment='top' );\n",
    "plt.subplot(2,2,3)\n",
    "bp = sns.boxplot( x='promo2_time_week', data=df5 );\n",
    "# Compare to Non-Zero Points\n",
    "aux = df5.loc[ df5['promo2_time_week']>0 , :].copy()\n",
    "plt.subplot(2,2,2)\n",
    "sns.distplot( aux['promo2_time_week'] );\n",
    "plt.xticks( fontsize=7 );\n",
    "plt.yticks( fontsize=7 );\n",
    "plt.annotate( 'Answer: better distribution!!', xy=(0.45, 0.68), xycoords='axes fraction', \n",
    "              xytext=(0.67, 0.95), textcoords='axes fraction',\n",
    "              arrowprops=dict(facecolor='red', shrink=0.05),\n",
    "              horizontalalignment='center', verticalalignment='top' );\n",
    "plt.subplot(2,2,4)\n",
    "bp = sns.boxplot( x='promo2_time_week', data=aux )\n",
    "\n",
    "# promo_time_week\n",
    "df5['promo2_time_week'] = mms.fit_transform( df5[['promo2_time_week']].values )\n",
    "pickle.dump( rs, open( '../webapp/parameter/promo_time_week_scaler.pkl', 'wb' ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.2 Rescaling: competition_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 Rescaling: competition_distance\n",
    "# Apply log, then apply min-max scaling\n",
    "plt.figure(figsize=(10, 4))\n",
    "#\n",
    "plt.subplot(2,3,1)\n",
    "sns.distplot( df5['competition_distance'] );\n",
    "plt.xticks( fontsize=7 );\n",
    "plt.yticks( rotation=60, fontsize=7 );\n",
    "plt.annotate( 'Original', xy=(0.45, 0.9), xycoords='axes fraction' )\n",
    "# \n",
    "plt.subplot(2,3,2)\n",
    "sns.distplot( np.sqrt( df5['competition_distance'] ) );\n",
    "plt.xticks( fontsize=7 );\n",
    "plt.yticks( rotation=60, fontsize=7 );\n",
    "plt.annotate( 'Sqrt()', xy=(0.45, 0.9), xycoords='axes fraction' )\n",
    "#\n",
    "plt.subplot(2,3,3)\n",
    "sns.distplot( np.log1p(df5['competition_distance']) );\n",
    "plt.xticks( fontsize=7 );\n",
    "plt.yticks( fontsize=7 );\n",
    "plt.annotate( 'Log()', xy=(0.45, 0.9), xycoords='axes fraction' )\n",
    "# Boxplot\n",
    "plt.subplot(2,3,4)\n",
    "bp = sns.boxplot( x='competition_distance', data=df5 );\n",
    "#\n",
    "plt.subplot(2,3,5)\n",
    "bp = sns.boxplot( x=np.sqrt( df5['competition_distance'] ) );\n",
    "\n",
    "# Apply LOG\n",
    "df5['competition_distance'] = np.log1p( df5['competition_distance'] )\n",
    "\n",
    "plt.subplot(2,3,6)\n",
    "bp = sns.boxplot( x='competition_distance', data=df5 )\n",
    "\n",
    "# Then apply MIN-MAX scaling\n",
    "df5['competition_distance'] = mms.fit_transform( df5[['competition_distance']].values )\n",
    "pickle.dump( mms, open( '../webapp/parameter/competition_distance_scaler.pkl', 'wb' ) )\n",
    "#pickle.dump( rs, open( 'parameter/competition_distance_scaler.pkl', 'wb' ) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.3 Rescaling: competition_time_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 Rescaling: competition_time_month\n",
    "# Apply log, then apply min-max scaling\n",
    "\n",
    "# let's disregard zeroes\n",
    "aux = df5.loc[ df5['competition_time_month']>0 , :].copy()\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(2,3,1)\n",
    "sns.distplot( df5['competition_time_month'] );\n",
    "plt.xticks( fontsize=7 );\n",
    "plt.yticks( rotation=60, fontsize=7 );\n",
    "plt.annotate( 'Original', xy=(0.45, 0.9), xycoords='axes fraction' )\n",
    "#\n",
    "plt.subplot(2,3,2)\n",
    "sns.distplot( np.sqrt( df5['competition_time_month'] ) );\n",
    "plt.xticks( fontsize=7 );\n",
    "plt.yticks( fontsize=7 );\n",
    "plt.annotate( 'Sqrt()', xy=(0.45, 0.9), xycoords='axes fraction' )\n",
    "#\n",
    "plt.subplot(2,3,3)\n",
    "sns.distplot( np.log1p(df5['competition_time_month']) );\n",
    "plt.xticks( fontsize=7 );\n",
    "plt.yticks( fontsize=7 );\n",
    "plt.annotate( 'Log()', xy=(0.45, 0.9), xycoords='axes fraction' )\n",
    "# Boxplot\n",
    "plt.subplot(2,3,4)\n",
    "bp = sns.boxplot( x='competition_time_month', data=df5 );\n",
    "#\n",
    "plt.subplot(2,3,5)\n",
    "bp = sns.boxplot( x=np.sqrt( df5['competition_time_month'] ) );\n",
    "#\n",
    "plt.subplot(2,3,6)\n",
    "bp = sns.boxplot( x=np.log1p( df5['competition_time_month'] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot again, no zeroes\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(2,3,1)\n",
    "sns.distplot( aux['competition_time_month'] );\n",
    "plt.xticks( fontsize=7 );\n",
    "plt.yticks( rotation=60, fontsize=7 );\n",
    "plt.annotate( 'Original - no zeroes', xy=(0.30, 0.9), xycoords='axes fraction' )\n",
    "#\n",
    "plt.subplot(2,3,2)\n",
    "sns.distplot( np.sqrt( aux['competition_time_month'] ) );\n",
    "plt.xticks( fontsize=7 );\n",
    "plt.yticks( fontsize=7 );\n",
    "plt.annotate( 'Sqrt()', xy=(0.45, 0.9), xycoords='axes fraction' )\n",
    "#\n",
    "plt.subplot(2,3,3)\n",
    "sns.distplot( np.log1p(aux['competition_time_month']) );\n",
    "plt.xticks( fontsize=7 );\n",
    "plt.yticks( fontsize=7 );\n",
    "plt.annotate( 'Log()', xy=(0.45, 0.9), xycoords='axes fraction' )\n",
    "# Boxplot\n",
    "plt.subplot(2,3,4)\n",
    "bp = sns.boxplot( x='competition_time_month', data=aux );\n",
    "#\n",
    "plt.subplot(2,3,5)\n",
    "bp = sns.boxplot( x=np.sqrt( aux['competition_time_month'] ) );\n",
    "\n",
    "# Apply LOG to aux\n",
    "aux['competition_time_month'] = np.log1p( aux['competition_time_month'] )\n",
    "\n",
    "plt.subplot(2,3,6)\n",
    "bp = sns.boxplot( x='competition_time_month', data=aux )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LOG\n",
    "df5['competition_time_month'] = np.log1p( df5['competition_time_month'] )\n",
    "\n",
    "# Then apply MIN-MAX scaling\n",
    "df5['competition_time_month'] = rs.fit_transform( df5[['competition_time_month']].values )\n",
    "pickle.dump( rs, open( '../webapp/parameter/competition_time_month_scaler.pkl', 'wb' ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.1 Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.3.1 Encoding\n",
    "\n",
    "# Categorical feature: STATE_HOLIDAY - one hot encoding\n",
    "df5 = pd.get_dummies( df5, prefix=['state_holiday'], columns=['state_holiday'] )\n",
    "\n",
    "# Categorical feature: STORE_TYPE - label encoding\n",
    "le = LabelEncoder()\n",
    "df5['store_type'] = le.fit_transform( df5['store_type'] ).astype( np.int64 )\n",
    "pickle.dump( le, open( '../webapp/parameter/store_type_scaler.pkl', 'wb' ) )\n",
    "\n",
    "# Categorical feature: ASSORTMENT (basic, extra, extended) - Ordinal encoding\n",
    "assortment_dict = {'basic': 1, 'extra': 2, 'extended': 3}\n",
    "df5['assortment'] = df5['assortment'].map( assortment_dict )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.2 Cyclical Features Encoding (Nature Transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.3.3 Nature Transformation\n",
    "# FEATURES WITH CYCLIC NATURE:\n",
    "\n",
    "# year\n",
    "df5['year'] = mms.fit_transform( df5[['year']].values )\n",
    "pickle.dump( mms, open( '../webapp/parameter/year_scaler.pkl', 'wb' ) )\n",
    "\n",
    "# month\n",
    "df5['month_sin'] = df5['month'].apply( lambda x: np.sin( x * ( 2. * np.pi/12 ) ) )\n",
    "df5['month_cos'] = df5['month'].apply( lambda x: np.cos( x * ( 2. * np.pi/12 ) ) )\n",
    "\n",
    "# day\n",
    "df5['day_sin'] = df5['day'].apply( lambda x: np.sin( x * ( 2. * np.pi/31 ) ) )\n",
    "df5['day_cos'] = df5['day'].apply( lambda x: np.cos( x * ( 2. * np.pi/31 ) ) )\n",
    "\n",
    "# week_of_year\n",
    "df5['week_of_year_sin'] = df5['week_of_year'].apply( lambda x: np.sin( x * ( 2. * np.pi/52.5 ) ) )\n",
    "df5['week_of_year_cos'] = df5['week_of_year'].apply( lambda x: np.cos( x * ( 2. * np.pi/52.5 ) ) )\n",
    "\n",
    "# day_of_week\n",
    "df5['day_of_week_sin'] = df5['day_of_week'].apply( lambda x: np.sin( x * ( 2. * np.pi/7 ) ) )\n",
    "df5['day_of_week_cos'] = df5['day_of_week'].apply( lambda x: np.cos( x * ( 2. * np.pi/7 ) ) )\n",
    "\n",
    "# quarter\n",
    "df5['quarter_sin'] = df5['quarter'].apply( lambda x: np.sin( x * ( 2. * np.pi/4 ) ) )\n",
    "df5['quarter_cos'] = df5['quarter'].apply( lambda x: np.cos( x * ( 2. * np.pi/4 ) ) )\n",
    "\n",
    "# 2-months\n",
    "df5['2months_sin'] = df5['2months'].apply( lambda x: np.sin( x * ( 2. * np.pi/6 ) ) )\n",
    "df5['2months_cos'] = df5['2months'].apply( lambda x: np.cos( x * ( 2. * np.pi/6 ) ) )\n",
    "\n",
    "# fortnight_of_year\n",
    "df5['fortnight_of_year_sin'] = df5['fortnight_of_year'].apply( lambda x: np.sin( x * ( 2. * np.pi/4 ) ) )\n",
    "df5['fortnight_of_year_cos'] = df5['fortnight_of_year'].apply( lambda x: np.cos( x * ( 2. * np.pi/4 ) ) )\n",
    "\n",
    "\n",
    "# ATENÇÃO: acho que deveria ter alguma transformação também para...\n",
    "# \n",
    "#    'competition_open_since_month',\n",
    "#    'competition_open_since_year',\n",
    "# \n",
    "#    'promo2_since_week',\n",
    "#    'promo2_since_year',\n",
    "#\n",
    "# Decisão: deixaremos para uma próxima rodada do CRISP\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3, 3));\n",
    "plt.scatter( df5['month_sin'], df5['month_cos'] );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.sample(6).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 6. FEATURE SELECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.0. Initial Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6 = df5.copy()\n",
    "df6.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. Split Train & Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1. Split dataframe into training and test set\n",
    "\n",
    "# First, before splitting, delete features already transformed to others\n",
    "\n",
    "# ATENÇÃO: isso já teria sido feito no passo 3 DATA CLEANING - VERIFICAR ISSO !!!\n",
    "\n",
    "cols_drop = [\n",
    "    'day', 'month', \n",
    "    'week_of_year', \n",
    "    'day_of_week',\n",
    "    'quarter', '2months',\n",
    "    'fortnight_of_year',\n",
    "    'promo2_since', \n",
    "    'competition_since', \n",
    "    'year_week']\n",
    "\n",
    "\n",
    "df6 = df6.drop( cols_drop, axis=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPLIT DATA: in time series analysis, do not split randomly, but split to test...\n",
    "# ...data from last six weeks.\n",
    "\n",
    "# TOTAL DATA: from 2013-jan-01 to 2015-jul-31\n",
    "\n",
    "# Limit-date: beginning of last six weeks = 2015-06-19\n",
    "print( df6[['store', 'date']].groupby('store').max().reset_index()['date'][0] - datetime.timedelta(days=6*7) )\n",
    "\n",
    "# Training dataset: total data minus six weeks, or: from 2013-jan-01 to 2015-06-18\n",
    "X_train = df6[df6['date'] < '2015-06-19']\n",
    "y_train = X_train['sales']\n",
    "\n",
    "# Test dataset: last six weeks, or: from 2015-06-19 to 2015-07-31\n",
    "X_test = df6[df6['date'] >= '2015-06-19']\n",
    "y_test = X_test['sales']\n",
    "\n",
    "print( 'Training Min Date: {}'.format( X_train['date'].min() ) )\n",
    "print( 'Training Max Date: {}'.format( X_train['date'].max() ) )\n",
    "\n",
    "print( '\\nTest Min Date: {}'.format( X_test['date'].min() ) )\n",
    "print( 'Test Max Date: {}'.format( X_test['date'].max() ) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. Running BORUTA as Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.1. Run Boruta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.2. Running BORUTA as Feature Selection\n",
    "\n",
    "# In case of error running BORUTA.FIT (module 'numpy' has no attribute 'int'), take a look at:\n",
    "# https://discuss.python.org/t/boruta-fit-error/23954/4\n",
    "# To downgrade NumPy from 1.26.0, USE: pip install --upgrade numpy==1.23.1\n",
    "# Or, alternatively, make np.int = np.int32 and np.float = np.float64 and np.bool = np.bool_\n",
    "# https://github.com/scikit-learn-contrib/boruta_py/issues/122\n",
    "\n",
    "# training and test dataset for Boruta (array)\n",
    "X_train_n = X_train.drop( ['date', 'sales'], axis=1 ).values\n",
    "y_train_n = y_train.values.ravel()\n",
    "\n",
    "# define RandomForestRegressor\n",
    "rf = RandomForestRegressor( n_jobs=-1 )\n",
    "\n",
    "# define Boruta [jump next line to avoid N hours waiting...]\n",
    "if BOL_RUN_BORUTA:\n",
    "    np.int = np.int32\n",
    "    np.float = np.float64\n",
    "    np.bool = np.bool_\n",
    "    boruta = BorutaPy( rf, n_estimators='auto', verbose=2, random_state=42 ).fit( X_train_n, y_train_n )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.2. Best Features from Boruta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.2.1. Best Features from Boruta\n",
    "\n",
    "if BOL_RUN_BORUTA:\n",
    "\n",
    "    # Select features BORUTA found as most relevants\n",
    "    cols_selected = boruta.support_.tolist()\n",
    "\n",
    "    # best features\n",
    "    X_train_fs = X_train.drop( ['date', 'sales'], axis=1 )\n",
    "    cols_selected_boruta = X_train_fs.iloc[:, cols_selected].columns.to_list()\n",
    "    print( cols_selected_boruta )\n",
    "\n",
    "    # COLS_SELECTED_BORUTA:\n",
    "    # ['store', 'promo', 'store_type', 'assortment',\n",
    "    #  'competition_distance', 'competition_open_since_month', 'competition_open_since_year',\n",
    "    #  'promo2', 'promo2_since_week', 'promo2_since_year', 'promo2_time_week',\n",
    "    #  'competition_time_month', 'month_cos', 'day_sin', 'day_cos', 'week_of_year_cos',\n",
    "    #  'day_of_week_sin', 'day_of_week_cos']\n",
    "\n",
    "    # not selected by boruta\n",
    "    cols_not_selected_boruta = list( np.setdiff1d( X_train_fs.columns, cols_selected_boruta ) )\n",
    "    print( cols_not_selected_boruta )\n",
    "\n",
    "    # COLS_NOT_SELECTED_BORUTA:\n",
    "    # ['is_promo2', '2months_sin', '2months_cos',\n",
    "    # 'fortnight_of_month', 'fortnight_of_year_sin', 'fortnight_of_year_cos',\n",
    "    # 'has_competition', 'quarter_sin', 'quarter_cos', 'semester', 'year',\n",
    "    # 'school_holiday', 'state_holiday_christmas', 'state_holiday_easter_holiday', \n",
    "    # 'state_holiday_public_holiday', 'state_holiday_regular_day' ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3. Manual Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 6.3. Manual Feature Selection\n",
    "\n",
    "# Compare features from BORUTA and from BIVARIATE ANALYSIS (4.2.9)\n",
    "\n",
    "# Features from Boruta\n",
    "cols_selected_boruta = [\n",
    "    'store',\n",
    "    'promo',\n",
    "    'store_type',\n",
    "    'assortment',\n",
    "    'competition_distance',\n",
    "    'competition_open_since_month',\n",
    "    'competition_open_since_year',\n",
    "    'promo2',\n",
    "    'promo2_since_week',\n",
    "    'promo2_since_year',\n",
    "    'competition_time_month',\n",
    "    'promo2_time_week',\n",
    "    'month_sin',  # we added this one\n",
    "    'month_cos',\n",
    "    'day_sin',\n",
    "    'day_cos',\n",
    "    'week_of_year_sin', # we added this one\n",
    "    'week_of_year_cos',\n",
    "    'day_of_week_sin',\n",
    "    'day_of_week_cos'\n",
    " ]\n",
    "\n",
    " # Columns to add\n",
    "feat_to_add = ['date','sales']\n",
    "\n",
    "# Full dataset, to use in Module 7 - class 33\n",
    "cols_selected_boruta_full = cols_selected_boruta.copy()\n",
    "cols_selected_boruta_full.extend( feat_to_add )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_not_selected_boruta = [\n",
    "    'is_promo2',\n",
    "    '2months_sin',\n",
    "    '2months_cos',\n",
    "    'fortnight_of_month',\n",
    "    'fortnight_of_year_sin',\n",
    "    'fortnight_of_year_cos',\n",
    "    'has_competition',\n",
    "    'quarter_sin',\n",
    "    'quarter_cos',\n",
    "    'semester',\n",
    "#    'month_sin',\n",
    "    'school_holiday',\n",
    "    'state_holiday_christmas',\n",
    "    'state_holiday_easter_holiday',\n",
    "    'state_holiday_public_holiday',\n",
    "    'state_holiday_regular_day',\n",
    "#    'week_of_year_sin',\n",
    "    'year'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features from Bivariate Analysis (4.2.1)\n",
    "print( tabulate( tab, headers='firstrow' ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Buvariate Analysis, the important hipothesys are H3, H8, H9, H10, H13, H14, H15, H16, H17\n",
    "\n",
    "# H3  - assortment           [boruta considered relevant]\n",
    "# H8  - christmas            [NOT relevant to boruta]\n",
    "# H9  - school_holiday       [NOT relevant to boruta]\n",
    "# H10 - state_holiday        [NOT relevant to boruta]\n",
    "# H13 - day_of_week          [boruta considered relevant]\n",
    "# H14 - month/state_holiday\n",
    "# H15 - month                [boruta just considered month_cos]\n",
    "# H16 - day                  [boruta considered relevant]\n",
    "# H17 - day\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 7. MACHINE LEARNING MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models are tested with their default hyperparameters.\n",
    "# The best performing model is used for 'fine tuning'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.0. Initial Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.0 PROCEDIMENTOS INICIAIS\n",
    "\n",
    "# Get features from BORUTA, excluding 'data' & 'sales'\n",
    "x_train = X_train[ cols_selected_boruta ]\n",
    "x_test = X_test[ cols_selected_boruta ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1. Average Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.1 AVERAGE MODEL\n",
    "\n",
    "baseline_result = 0\n",
    "\n",
    "if BOL_RUN_MLMODELS:\n",
    "\n",
    "    aux1 = x_test.copy()\n",
    "    aux1['sales'] = y_test.copy()\n",
    "\n",
    "    # prediction\n",
    "    aux2 = aux1[['store', 'sales']].groupby('store').mean().reset_index().rename( columns={'sales': 'predictions'} )\n",
    "    aux1 = pd.merge( aux1, aux2, how='left', on='store' )\n",
    "    yhat_baseline = aux1['predictions']\n",
    "\n",
    "    # performance\n",
    "    # PS: in 5.3.2 we made LOG(sales). Undoing now...\n",
    "    baseline_result = ml_error( 'Average Model', np.expm1( y_test ), np.expm1( yhat_baseline ) )\n",
    "\n",
    "baseline_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2. Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.2 LINEAR REGRESSION MODEL\n",
    "\n",
    "lr_result = 0\n",
    "\n",
    "if BOL_RUN_MLMODELS:\n",
    "\n",
    "    # model\n",
    "    lr = LinearRegression().fit( x_train, y_train )\n",
    "\n",
    "    # prediction\n",
    "    yhat_lr = lr.predict( x_test )\n",
    "\n",
    "    # performance\n",
    "    lr_result = ml_error( 'Linear Regression', np.expm1( y_test ), np.expm1( yhat_lr ) )\n",
    "\n",
    "lr_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3. Linear Regression Regularized Model - Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.3. Linear Regression Regularized Model - Lasso\n",
    "\n",
    "lrr_result = 0\n",
    "\n",
    "if BOL_RUN_MLMODELS:\n",
    "\n",
    "    # model\n",
    "    lrr = Lasso( alpha=0.01 ).fit( x_train, y_train )\n",
    "\n",
    "    # prediction\n",
    "    yhat_lrr = lrr.predict( x_test )\n",
    "\n",
    "    # performance\n",
    "    lrr_result = ml_error( 'Linear Regression - Lasso', np.expm1( y_test ), np.expm1( yhat_lrr ) )\n",
    "\n",
    "lrr_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4. Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.4. Random Forest Regressor\n",
    "\n",
    "rf_result = 0\n",
    "\n",
    "if BOL_RUN_MLMODELS:\n",
    "\n",
    "    # model\n",
    "    rf = RandomForestRegressor( n_estimators=100, n_jobs=-1, random_state=42 ).fit( x_train, y_train )\n",
    "\n",
    "    # prediction\n",
    "    yhat_rf = rf.predict( x_test )\n",
    "\n",
    "    # performance\n",
    "    rf_result = ml_error( 'Random Forest Regressor', np.expm1( y_test ), np.expm1( yhat_rf ) )\n",
    "\n",
    "rf_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5. XGBoost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.5. XGBOOST REGRESSOR\n",
    "\n",
    "xgb_result = 0\n",
    "\n",
    "if BOL_RUN_MLMODELS:\n",
    "\n",
    "    # model\n",
    "    model_xgb = xgb.XGBRegressor( objective='reg:squarederror',\n",
    "                                n_estimators=100, \n",
    "                                #eta=0.01,\n",
    "                                max_depth=10,\n",
    "                                subsample=0.7,\n",
    "                                colsample_bytee=0.9 ).fit( x_train, y_train )\n",
    "\n",
    "    # prediction\n",
    "    yhat_xgb = model_xgb.predict( x_test )\n",
    "\n",
    "    # performance\n",
    "    xgb_result = ml_error( 'XGBoost Regressor', np.expm1( y_test ), np.expm1( yhat_xgb ) )\n",
    "\n",
    "xgb_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.6. Compare Models' Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.6 COMPARE MODELS' PERFORMANCE\n",
    "\n",
    "# PS: this result is not as reliable as \"cross-validation\"\n",
    "\n",
    "modelling_result = 0\n",
    "\n",
    "if BOL_RUN_MLMODELS:\n",
    "    modelling_result = pd.concat( [ baseline_result, lr_result, lrr_result, rf_result, xgb_result ] )\n",
    "    modelling_result = modelling_result.sort_values( 'RMSE' )\n",
    "\n",
    "modelling_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add data & sales:\n",
    "x_training = X_train[ cols_selected_boruta_full ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = LinearRegression()\n",
    "a = 0\n",
    "\n",
    "if BOL_RUN_MLMODELS:\n",
    "    model = Lasso( alpha=0.01 )\n",
    "    a = cross_validation( x_training, 5, 'Lasso', model, True )\n",
    "\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.7. Compare Models' Cross-Validation Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.7.1. Linear Regression Model - Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.7.1. Linear Regression Model - Cross Validation\n",
    "# Performance without cross validation\n",
    "lr_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.7.1 Linear Regression Model - Cross Validation\n",
    "lr_result_cv = 0\n",
    "\n",
    "if BOL_RUN_MLMODELS:\n",
    "    lr_result_cv = cross_validation( x_training, 5, 'Linear Regression', lr, False )\n",
    "\n",
    "lr_result_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.7.2. Lasso Model - Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.7.2. Lasso Model - cv\n",
    "# Performance without cv\n",
    "lrr_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.7.1 Linear Regression Model - Cross Validation\n",
    "lrr_result_cv = 0\n",
    "\n",
    "if BOL_RUN_MLMODELS:\n",
    "    lrr_result_cv = cross_validation( x_training, 5, 'Lin.Regr. Lasso', lrr, False )\n",
    "\n",
    "lrr_result_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.7.3. Random Forest Model - Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.7.3. Random Forest Model - cv\n",
    "# Performance without cv\n",
    "rf_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.7.3 Random Forest Model - Cross Validation\n",
    "rf_result_cv = 0\n",
    "\n",
    "if BOL_RUN_MLMODELS:\n",
    "    rf_result_cv = cross_validation( x_training, 5, 'Random Forest Regressor CV', rf, True )\n",
    "\n",
    "rf_result_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.7.4. XGBoost Model - Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.7.4. XGBoost Model - cv\n",
    "# Performance without cv\n",
    "xgb_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.7.1 XGBoost Model - Cross Validation\n",
    "xgb_result_cv = 0\n",
    "\n",
    "if BOL_RUN_MLMODELS:\n",
    "    xgb_result_cv = cross_validation( x_training, 5, 'XGBoost Regressor CV', model_xgb, True )\n",
    "\n",
    "xgb_result_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.8. Compare Models' Performance (cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.8.1. Single Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelling_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.8.2. Real Model Performance - with Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelling_result_cv = 0\n",
    "\n",
    "if BOL_RUN_MLMODELS:\n",
    "    modelling_result_cv = pd.concat( [ lr_result_cv, lrr_result_cv, rf_result_cv, xgb_result_cv ] )\n",
    "    modelling_result_cv = modelling_result_cv.sort_values( 'RMSE CV' )\n",
    "    print()\n",
    "\n",
    "modelling_result_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 8. HYPERPARAMETER FINE TUNING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.0. Initial Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOP..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1. Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    #'n_estimators': [15, 17, 30, 35],\n",
    "    'n_estimators': [1000],\n",
    "    #'eta': [0.01, 0.03],\n",
    "    'max_depth': [3, 5, 9],\n",
    "    'subsample': [0.1, 0.5, 0.7],\n",
    "    'min_child_weight': [3, 8, 15],\n",
    "    'colsample_bytee': [0.3]\n",
    "}\n",
    "\n",
    "# Number of iterations\n",
    "MAX_EVAL = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    'n_estimators': [1000],\n",
    "    'max_depth': [5],\n",
    "    'subsample': [0,75, 0.80, 0.85],\n",
    "    'min_child_weight': [15],\n",
    "    'colsample_bytee': [0.3]\n",
    "}\n",
    "\n",
    "# Number of iterations\n",
    "MAX_EVAL = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result = pd.DataFrame()\n",
    "first_time = True\n",
    "best_result = None\n",
    "best_hp = []\n",
    "\n",
    "if BOL_RUN_FINETUNING:\n",
    "\n",
    "    for i in range( MAX_EVAL ):\n",
    "        # Random parameters\n",
    "        hp = { k: random.sample( v, 1 )[0] for k, v in param.items() }\n",
    "        print( hp )\n",
    "\n",
    "        # model\n",
    "        model_xgb = xgb.XGBRegressor( objective='reg:squarederror',\n",
    "                                    n_estimators=hp['n_estimators'], \n",
    "                                    #eta=hp['eta'],\n",
    "                                    max_depth=hp['max_depth'],\n",
    "                                    subsample=hp['subsample'],\n",
    "                                    min_child_weight=hp['min_child_weight'],\n",
    "                                    colsample_bytee=hp['colsample_bytee'] )\n",
    "\n",
    "        # performance\n",
    "        result = cross_validation( x_training, 2, 'XGBoost Regressor', model_xgb )\n",
    "        rmse = result.loc[ 0, 'RMSE CV' ]\n",
    "        final_result = pd.concat( [final_result, result] )\n",
    "        if first_time:\n",
    "            first_time = False\n",
    "            best_result = result\n",
    "        else:\n",
    "            if result.loc[ 0, 'rmse_float' ] < best_result.loc[ 0, 'rmse_float' ]:\n",
    "                best_result = result\n",
    "                print('best result')\n",
    "                print(result)\n",
    "                print('best hp')\n",
    "                print(hp)\n",
    "                best_hp = [ hp['n_estimators'], hp['max_depth'], hp['subsample'], hp['min_child_weight'], hp['colsample_bytee'] ]\n",
    "\n",
    "    print('----- BEST RESULTS -----')\n",
    "    print(best_result)\n",
    "    print(best_hp)\n",
    "\n",
    "final_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2. Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results in dd.mm.aaaa:\n",
    "# in 31.1.2024: rmse=1284.7  {'n_estimators': 1000, 'max_depth': 9, 'subsample': 0.5, 'min_child_weight': 15, 'colsample_bytee': 0.3}\n",
    "# in DD.1.2024: rmse=1263.47 {'n_estimators': 1000, 'max_depth': 5, 'subsample': 0.7, 'min_child_weight': 15, 'colsample_bytee': 0.3}\n",
    "# in 06.2.2024: rmse=1249.15 {'n_estimators': 1000, 'max_depth': 5, 'subsample': 0.8, 'min_child_weight': 15, 'colsample_bytee': 0.3}\n",
    "# in 07.2.2024: rmse=1252.60 {'n_estimators': 1000, 'max_depth': 5, 'subsample': 1.0, 'min_child_weight': 15, 'colsample_bytee': 0.3}\n",
    "\n",
    "param_tuned = {\n",
    "    'n_estimators': 1000,  # RMSE = 1249.15\n",
    "    #'eta': 0.03,\n",
    "    'max_depth': 5,\n",
    "    'subsample': 0.8,\n",
    "    'min_child_weight': 15,\n",
    "    'colsample_bytee': 0.3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "model_xgb_tuned = xgb.XGBRegressor( objective='reg:squarederror',\n",
    "                                    n_estimators=param_tuned['n_estimators'], \n",
    "                                    #eta=param_tuned['eta'],\n",
    "                                    max_depth=param_tuned['max_depth'],\n",
    "                                    subsample=param_tuned['subsample'],\n",
    "                                    min_child_weight=param_tuned['min_child_weight'],\n",
    "                                    colsample_bytee=param_tuned['colsample_bytee'] ).fit(x_train, y_train)\n",
    "\n",
    "# prediction\n",
    "yhat_xgb_tuned = model_xgb_tuned.predict( x_test )\n",
    "\n",
    "# performance\n",
    "xgb_result_tuned = ml_error( 'XGBoost Regressor', np.expm1( y_test ), np.expm1( yhat_xgb_tuned ) )\n",
    "\n",
    "# save (pickle)\n",
    "pickle.dump( model_xgb_tuned, open( '../webapp/model/model_rossmann.pkl', 'wb' ) )\n",
    "\n",
    "xgb_result_tuned\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 9. ERROR TRANSLATION & INTERPRETATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.0. Initial Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. ERROR TRANSLATION - initial procedure\n",
    "\n",
    "# copy dataset: only last six weeks, from 19-6 to 31-7-2015\n",
    "df9 = X_test[ cols_selected_boruta_full ]\n",
    "\n",
    "# Transform output variables (sales & predictions) back to normal (inv.log == expm1).\n",
    "df9['sales'] = np.expm1( df9['sales'] )\n",
    "df9['predictions'] = np.expm1( yhat_xgb_tuned )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df9.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1. Business Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.1. ERROR TRANSLATION - Business Performance\n",
    "\n",
    "# Analysis per store units\n",
    "\n",
    "# SUM OF PREDICTIONS per store (over six weeks, from 19-06 to 31-07-2015)\n",
    "df91  = df9[['store', 'predictions']].groupby('store').sum().reset_index()\n",
    "df91A = df9[['store', 'promo']].groupby('store').count().reset_index()\n",
    "df91A.columns = ['store', 'ndays']\n",
    "\n",
    "# MAE per store (over six weeks, from 19-06 to 31-07-2015)\n",
    "df9_aux1 = (df9[['store', 'sales', 'predictions']]\n",
    "            .groupby( 'store' )\n",
    "            .apply( lambda x: mean_absolute_error( x['sales'], x['predictions'] ) )\n",
    "            .reset_index().rename(columns={0: 'MAE'}))\n",
    "\n",
    "# MAPE per store (over six weeks, from 19-06 to 31-07-2015)\n",
    "df9_aux2 = (df9[['store', 'sales', 'predictions']]\n",
    "            .groupby( 'store' )\n",
    "            .apply( lambda x: mean_absolute_percentage_error( x['sales'], x['predictions'] ) )\n",
    "            .reset_index().rename(columns={0: 'MAPE'}))\n",
    "\n",
    "# Merge\n",
    "df9_aux3 = pd.merge(df9_aux1, df9_aux2, how='inner', on='store')\n",
    "df92 = pd.merge( df91, df9_aux3, how='inner', on='store' )\n",
    "df92 = pd.merge( df92, df91A, how='inner', on='store' )\n",
    "\n",
    "# Scenarios\n",
    "df92['best_scenario']  = df92['predictions'] + df92['ndays'] * df92['MAE']\n",
    "df92['worst_scenario'] = df92['predictions'] - df92['ndays'] * df92['MAE']\n",
    "df92['worst_scenario'] = df92['worst_scenario'].apply( lambda x: 0 if x<0 else x )\n",
    "\n",
    "# Order columns\n",
    "df92 = df92[['store', 'predictions', 'worst_scenario', 'best_scenario', 'MAE', 'MAPE', 'ndays']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.1. ERROR TRANSLATION - Business Performance\n",
    "# Which store units have higher forecasting errors?\n",
    "print( df92.sort_values( 'MAPE', ascending=False ).head().round(2) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( df92.sort_values( 'MAPE', ascending=True ).head().round(2) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.1. ERROR TRANSLATION - Business Performance\n",
    "plt.figure( figsize=(10, 4) )\n",
    "sns.scatterplot( x='store', y='MAPE', data=df92 );\n",
    "plt.annotate( 'Mean Absolute Percentage Error (MAPE) - per Store', xy=(0.25, 0.80), xycoords='axes fraction', size=12 );\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2. Total Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.2. ERROR TRANSLATION - Total Performance\n",
    "\n",
    "# df92[['predictions', 'worst_scenario', 'best_scenario']].sum()\n",
    "df93 = (df92[['predictions', 'worst_scenario', 'best_scenario']]\n",
    "        .apply( lambda x: np.sum( x ), axis=0 )\n",
    "        .reset_index()\n",
    "        .rename(columns={'index': 'Scenario', 0: 'Values'}))\n",
    "\n",
    "df93['Values'] = df93['Values'].map( 'R${:,.2f}'.format )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df93"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3. Machine Learning Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.3. ERROR TRANSLATION - Machine Learning Performance\n",
    "\n",
    "df9['error'] = df9['sales'] - df9['predictions']\n",
    "df9['error_rate'] = df9['predictions'] / df9['sales']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure( figsize=(10, 7) )\n",
    "plt.subplot( 2, 2, 1 )\n",
    "sns.lineplot( x='date', y='sales', data=df9, label='SALES' )\n",
    "sns.lineplot( x='date', y='predictions', data=df9, label='PREDICTIONS' )\n",
    "#plt.title('SALES & PREDICTIONS')\n",
    "\n",
    "plt.subplot( 2, 2, 3 )\n",
    "sns.lineplot( x='date', y='error_rate', data=df9, label='ERROR RATE' )\n",
    "plt.axhline( 1, linestyle='--' )\n",
    "\n",
    "plt.annotate( 'prediction > sales', xy=(0.4, 0.7), xycoords='axes fraction', \n",
    "              xytext=(0.6, 0.85), textcoords='axes fraction', size=8,\n",
    "              arrowprops=dict(facecolor='red', shrink=0.05),\n",
    "              horizontalalignment='center', verticalalignment='top' );\n",
    "\n",
    "plt.annotate( 'prediction < sales', xy=(0.4, 0.4), xycoords='axes fraction', \n",
    "              xytext=(0.2, 0.3), textcoords='axes fraction', size=8,\n",
    "              arrowprops=dict(facecolor='red', shrink=0.05),\n",
    "              horizontalalignment='center', verticalalignment='top' );\n",
    "\n",
    "plt.subplot( 2, 2, 2 )\n",
    "sns.distplot( df9['error'] );\n",
    "plt.xlim(-10000, 10000)\n",
    "plt.annotate( 'sales > prediction', xy=(0.7, 0.6), xycoords='axes fraction', size=8 );\n",
    "plt.annotate( 'sales < prediction', xy=(0.1, 0.6), xycoords='axes fraction', size=8 );\n",
    "\n",
    "plt.subplot( 2, 2, 4 )\n",
    "sns.scatterplot( x=df9['predictions'], y=df9['error'] );\n",
    "plt.annotate( 'ERROR x PREDICTION', xy=(0.35, 0.85), xycoords='axes fraction', size=8 );\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 10. DEPLOY MODEL TO PRODUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.0. Initial procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving trained model\n",
    "# this file will be used by the API, in production.\n",
    "pickle.dump( model_xgb_tuned, open( '../webapp/model/model_rossmann.pkl', 'wb' ) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.1. Rossmann Prediction Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import inflection\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import datetime\n",
    "\n",
    "class Rossmann( object ):\n",
    "    def __init__( self ):\n",
    "        self.home_path = 'c:/MeusEstudos/CURSOS TI/Em 2023 - ComunidadeDS/F2.D2 - DS.Prod/Projeto/'\n",
    "        self.competition_distance_scaler   = pickle.load( open( self.home_path + 'parameter/competition_distance_scaler.pkl', 'rb' ) )\n",
    "        self.competition_time_month_scaler = pickle.load( open( self.home_path + 'parameter/competition_time_month_scaler.pkl', 'rb' ) )\n",
    "        self.promo_time_week_scaler        = pickle.load( open( self.home_path + 'parameter/promo_time_week_scaler.pkl', 'rb' ) )\n",
    "        self.year_scaler                   = pickle.load( open( self.home_path + 'parameter/year_scaler.pkl', 'rb' ) )\n",
    "        self.store_type_scaler             = pickle.load( open( self.home_path + 'parameter/store_type_scaler.pkl', 'rb' ) )\n",
    "\n",
    "\n",
    "    def data_cleaning(self, df1):\n",
    "        # Copied from STEP-1, items 1.2, 1.4, 1.6 and 1.7\n",
    "\n",
    "        # 1.2. RENAME COLUMNS\n",
    "        cols_old = ['Store', 'DayOfWeek', 'Date', 'Open', 'Promo',\n",
    "                    'StateHoliday', 'SchoolHoliday', 'StoreType', 'Assortment',\n",
    "                    'CompetitionDistance', 'CompetitionOpenSinceMonth',\n",
    "                    'CompetitionOpenSinceYear', 'Promo2', 'Promo2SinceWeek',\n",
    "                    'Promo2SinceYear', 'PromoInterval' ]\n",
    "\n",
    "        snakecase = lambda x: inflection.underscore( x )\n",
    "        cols_new = list( map( snakecase, cols_old ) )\n",
    "\n",
    "        # rename\n",
    "        df1.columns = cols_new\n",
    "\n",
    "        # 1.4. Data Types\n",
    "        df1['date'] = pd.to_datetime( df1['date'] )\n",
    "\n",
    "        # 1.6. FILLOUT NA\n",
    "\n",
    "        # competition_distance\n",
    "        df1['competition_distance'] = (df1['competition_distance']\n",
    "                                       .apply( lambda x: 200000.0 if math.isnan( x ) else x ))\n",
    "\n",
    "        # competition_open_since_month\n",
    "        df1['competition_open_since_month'] = (\n",
    "            df1.apply( lambda x: x['date'].month if math.isnan( x['competition_open_since_month'] ) else x['competition_open_since_month'], axis=1 )\n",
    "        )\n",
    "\n",
    "        # competition_open_since_year\n",
    "        df1['competition_open_since_year'] = (\n",
    "            df1.apply( lambda x: x['date'].year if math.isnan( x['competition_open_since_year'] ) else x['competition_open_since_year'], axis=1 )\n",
    "        )\n",
    "\n",
    "        # promo2_since_week\n",
    "        df1['promo2_since_week'] = (\n",
    "            df1.apply( lambda x: x['date'].week if math.isnan( x['promo2_since_week'] ) else x['promo2_since_week'], axis=1 )\n",
    "        )\n",
    "\n",
    "        # promo2_since_year\n",
    "        df1['promo2_since_year'] = (\n",
    "            df1.apply( lambda x: x['date'].year if math.isnan( x['promo2_since_year'] ) else x['promo2_since_year'], axis=1 )\n",
    "        )\n",
    "\n",
    "        # promo_interval\n",
    "        df1['promo_interval'].fillna( 0, inplace=True )\n",
    "\n",
    "        # 1.7. CHANGE TYPES\n",
    "        # From FLOAT to INT\n",
    "        df1['competition_open_since_month'] = df1['competition_open_since_month'].astype( np.int64 )\n",
    "        df1['competition_open_since_year'] = df1['competition_open_since_year'].astype( np.int64 )\n",
    "        df1['promo2_since_week']          = df1['promo2_since_week'].astype( np.int64 )\n",
    "        df1['promo2_since_year']         = df1['promo2_since_year'].astype( np.int64 )\n",
    "\n",
    "        return df1\n",
    "\n",
    "    def feature_engineering(self, df2):\n",
    "        # Copied from STEP-2, items 2.4.1 to 2.4.5, 3.1 and 3.2\n",
    "\n",
    "        # 2.4.1 Attributes derived from 'PROMO2'\n",
    "\n",
    "        # Attribute PROMO2: force to ZERO if date < (promo2_since_week + promo2_since_year)\n",
    "        # PS: this is necessary because 'promo2' is originally a STORE.CSV attribute, not a TRAIN.CSV attribute\n",
    "        # Example: STORE-28 started its 'promo2' only in 2015, 6th week. Before this week, promo2 should be ZERO.\n",
    "        df2['promo2aux'] = df2.apply( lambda x: 0 if x['promo2'] == 0 \n",
    "                                                    or x['date'].year < x['promo2_since_year'] \n",
    "                                                    or (x['date'].year == x['promo2_since_year'] and x['date'].week < x['promo2_since_week']) \n",
    "                                                else 1, axis=1 )\n",
    "\n",
    "        # Attribute \"IS_PROMO2\"\n",
    "        # From \"promo2\" & \"promo_interval\", create \"is_promo2\" attribute\n",
    "\n",
    "        month_map = { 1:'Jan', 2:'Feb', 3:'Mar', 4:'Apr', 5:'May', 6:'Jun', 7:'Jul', 8:'Aug', 9:'Sept', 10:'Oct', 11:'Nov', 12:'Dec' }\n",
    "\n",
    "        df2['month_map'] = df2['date'].dt.month.map( month_map )\n",
    "\n",
    "        df2['is_promo2'] = ( df2[['promo_interval', 'promo2aux', 'month_map']]\n",
    "                            .apply( lambda x: 0 if x['promo2aux'] == 0 \n",
    "                                                else 1 if x['month_map'] in x['promo_interval'].split( ',' ) \n",
    "                                                else 0, axis=1 ) )\n",
    "        # Attribute PROMO2_SINCE\n",
    "        # PS: using features: \"promo2\", \"since_week\" and \"since_year\"\n",
    "        df2['promo2_since'] = df2['promo2_since_year'].astype( str ) + '-' + df2['promo2_since_week'].astype( str )\n",
    "        df2['promo2_since'] = df2['promo2_since'].apply( \n",
    "            lambda x: datetime.datetime.strptime( x + '-1', '%Y-%W-%w' ) - datetime.timedelta( days=7 ) \n",
    "        )\n",
    "\n",
    "        # Attribute PROMO2_TIME_WEEK\n",
    "        df2['promo2_time_week'] = ( ( df2['date'] - df2['promo2_since'] )/7 ).apply( lambda x: x.days ).astype( np.int64 )\n",
    "        df2['promo2_time_week'] = df2.apply( lambda x: 0.0 if x['promo2_time_week'] < 0 else x['promo2_time_week'], axis=1 )\n",
    "\n",
    "        # 2.4.2 Attributes derived from 'COMPETITION'\n",
    "\n",
    "        # Attribute HAS_COMPETITION: =ZERO in the days before CompetitionOpenSince[month/year].\n",
    "        #                            =ONE after that date.\n",
    "        df2['has_competition'] = ( df2\n",
    "            .apply( lambda x: 0 if x['date'].year < x['competition_open_since_year'] \n",
    "                                or (x['date'].year == x['competition_open_since_year'] and x['date'].month < x['competition_open_since_month']) \n",
    "                                else 1, axis=1 ) )\n",
    "\n",
    "        # Attribute COMPETITION_DISTANCE\n",
    "        df2['competition_distance'] = df2.apply( lambda x: 200000.0 if x['has_competition'] == 0 else x['competition_distance'], axis=1 )\n",
    "\n",
    "        # Attribute COMPETITION_SINCE\n",
    "        # PS: info separated in MONTH & YEAR. Join them and calculate date difference to current date\n",
    "        df2['competition_since'] = df2.apply( \n",
    "            lambda x: datetime.datetime( \n",
    "                year=x['competition_open_since_year'], \n",
    "                month=x['competition_open_since_month'], day=1 ), \n",
    "            axis=1 )\n",
    "        df2['competition_time_month'] = ( ( df2['date'] - df2['competition_since'] )/30 ).apply( lambda x: x.days ).astype( np.int64 )\n",
    "        df2['competition_time_month'] = df2.apply( lambda x: 0.0 if x['competition_time_month'] < 0 else x['competition_time_month'], axis=1 )\n",
    "\n",
    "        # 2.4.3 Attributes: time periodicity\n",
    "\n",
    "        # year\n",
    "        df2['year'] = df2['date'].dt.year\n",
    "        # month\n",
    "        df2['month'] = df2['date'].dt.month\n",
    "        # day\n",
    "        df2['day'] = df2['date'].dt.day\n",
    "\n",
    "        # semester\n",
    "        df2['semester'] = df2['month'].apply( lambda x: 0 if x < 7 else 1 ).astype( np.int64 )\n",
    "\n",
    "        # quarter\n",
    "        df2['quarter'] = df2['month'].apply( lambda x: 1 if x < 4 \n",
    "                                                else 2 if x < 7 \n",
    "                                                else 3 if x < 10 \n",
    "                                                else 4 ).astype( np.int64 )\n",
    "        # two months\n",
    "        df2['2months'] = df2['month'].apply( lambda x: 1 if x < 3 \n",
    "                                                else 2 if x < 5 \n",
    "                                                else 3 if x < 7 \n",
    "                                                else 4 if x < 9 \n",
    "                                                else 5 if x < 11 \n",
    "                                                else 6 ).astype( np.int64 )\n",
    "        # fortnight\n",
    "        df2['fortnight_of_year'] = df2.apply( lambda x: (2* x['month']) if x['day'] > 15 \n",
    "                                                else (2* x['month'] -1), axis=1 ).astype( np.int64 )\n",
    "        df2['fortnight_of_month'] = df2.apply( lambda x: 1 if x['day'] > 15 else 0, axis=1)\n",
    "\n",
    "        # week of year\n",
    "        # PS: function \"weekofyear\" didn't work. Replaced for the following:\n",
    "        # REF: https://saturncloud.io/blog/how-to-convert-a-pandas-date-to-week-number-in-python/\n",
    "        #df2['week_of_year'] = df2['date'].dt.weekofyear\n",
    "        df2['week_of_year'] = df2['date'].dt.strftime('%U').astype(int) + 1\n",
    "\n",
    "        # year week\n",
    "        df2['year_week'] = df2['date'].dt.strftime( '%Y-%W' )\n",
    "\n",
    "        # 2.4.4 Attributes: \"assortment\" & \"state_holiday\"\n",
    "\n",
    "        # assortment: a=basic, b=extra, c=extended\n",
    "        df2['assortment'] = ( df2['assortment']\n",
    "                             .apply( lambda x: 'basic' if x == 'a' \n",
    "                                          else 'extra' if x == 'b' \n",
    "                                          else 'extended' ) )\n",
    "\n",
    "        # state holiday\n",
    "        # a=public holiday, b=Easter holiday, c=Christmas, 0=regular working day\n",
    "        df2['state_holiday'] = df2['state_holiday'].apply( \n",
    "            lambda x: 'public_holiday' if x == 'a' else \n",
    "                    'easter_holiday' if x == 'b' else \n",
    "                    'christmas' if x == 'c' else \n",
    "                    'regular_day' )\n",
    "\n",
    "        # 2.4.5 Attribute: \"sales_per_customer\"\n",
    "        df2['sales_per_customer'] = (df2\n",
    "                        .apply( lambda x: 0 if x['customers'] == 0 \n",
    "                                          else x['sales'] / x['customers'], axis=1 ) )\n",
    "\n",
    "        # 3.1. ROWS FILTERING\n",
    "        df2 = df2[(df2['open'] != 0) & (df2['sales'] > 0)]\n",
    "\n",
    "        # 3.2. COLUMNS FILTERING\n",
    "        cols_drop = [ 'open', 'promo_interval', 'month_map', 'promo2aux' ]\n",
    "        df2 = df2.drop( cols_drop, axis=1 )\n",
    "\n",
    "        return df2\n",
    "\n",
    "    def data_preparetion(self, df5):\n",
    "        # Code copied from STEP-5/6, items 5.1, 5.2, 5.3, 6.3\n",
    "\n",
    "        # 5.1. Response Variable Normalization\n",
    "        df5['sales'] = np.log1p( df5['sales'] )\n",
    "\n",
    "        # 5.2. DATA PREPARATION - Rescaling\n",
    "        # promo2_time_week\n",
    "        df5['promo2_time_week'] = self.promo_time_week_scaler.fit_transform( df5[['promo2_time_week']].values )\n",
    "        # competition_distance\n",
    "        df5['competition_distance'] = self.competition_distance_scaler.fit_transform( df5[['competition_distance']].values )\n",
    "        # competition_time_month\n",
    "        df5['competition_time_month'] = self.competition_time_month_scaler.fit_transform( df5[['competition_time_month']].values )\n",
    "\n",
    "        # 5.3.1 Encoding\n",
    "        # Categorical feature: state_holiday - one hot encoding\n",
    "        df5 = pd.get_dummies( df5, prefix=['state_holiday'], columns=['state_holiday'] )\n",
    "        # Categorical feature: store_type - label encoding\n",
    "        df5['store_type'] = self.store_type_scaler.fit_transform( df5['store_type'] ).astype( np.int64 )\n",
    "        # Categorical feature: assortment (basic, extra, extended) - Ordinal encoding\n",
    "        assortment_dict = {'basic': 1, 'extra': 2, 'extended': 3}\n",
    "        df5['assortment'] = df5['assortment'].map( assortment_dict )\n",
    "\n",
    "        # 5.3.2 Nature Transformation\n",
    "        # year\n",
    "        df5['year'] = self.year_scaler.fit_transform( df5[['year']].values )\n",
    "        # CYCLICAL NATURE FEATURES:\n",
    "        # month\n",
    "        df5['month_sin'] = df5['month'].apply( lambda x: np.sin( x * ( 2. * np.pi/12 ) ) )\n",
    "        df5['month_cos'] = df5['month'].apply( lambda x: np.cos( x * ( 2. * np.pi/12 ) ) )\n",
    "        # day\n",
    "        df5['day_sin'] = df5['day'].apply( lambda x: np.sin( x * ( 2. * np.pi/31 ) ) )\n",
    "        df5['day_cos'] = df5['day'].apply( lambda x: np.cos( x * ( 2. * np.pi/31 ) ) )\n",
    "        # week_of_year\n",
    "        df5['week_of_year_sin'] = df5['week_of_year'].apply( lambda x: np.sin( x * ( 2. * np.pi/52.5 ) ) )\n",
    "        df5['week_of_year_cos'] = df5['week_of_year'].apply( lambda x: np.cos( x * ( 2. * np.pi/52.5 ) ) )\n",
    "        # day_of_week\n",
    "        df5['day_of_week_sin'] = df5['day_of_week'].apply( lambda x: np.sin( x * ( 2. * np.pi/7 ) ) )\n",
    "        df5['day_of_week_cos'] = df5['day_of_week'].apply( lambda x: np.cos( x * ( 2. * np.pi/7 ) ) )\n",
    "        # quarter\n",
    "        df5['quarter_sin'] = df5['quarter'].apply( lambda x: np.sin( x * ( 2. * np.pi/4 ) ) )\n",
    "        df5['quarter_cos'] = df5['quarter'].apply( lambda x: np.cos( x * ( 2. * np.pi/4 ) ) )\n",
    "        # 2-months\n",
    "        df5['2months_sin'] = df5['2months'].apply( lambda x: np.sin( x * ( 2. * np.pi/6 ) ) )\n",
    "        df5['2months_cos'] = df5['2months'].apply( lambda x: np.cos( x * ( 2. * np.pi/6 ) ) )\n",
    "        # fortnight_of_year\n",
    "        df5['fortnight_of_year_sin'] = df5['fortnight_of_year'].apply( lambda x: np.sin( x * ( 2. * np.pi/4 ) ) )\n",
    "        df5['fortnight_of_year_cos'] = df5['fortnight_of_year'].apply( lambda x: np.cos( x * ( 2. * np.pi/4 ) ) )\n",
    "\n",
    "        # 6.3. Features from Boruta\n",
    "        cols_selected = [ 'store', 'promo', 'store_type', 'assortment', 'competition_distance',\n",
    "            'competition_open_since_month', 'competition_open_since_year', 'promo2',\n",
    "            'promo2_since_week', 'promo2_since_year', 'competition_time_month',\n",
    "            'promo2_time_week', 'month_sin', 'month_cos', 'day_sin', 'day_cos', 'week_of_year_sin',\n",
    "            'week_of_year_cos', 'day_of_week_sin', 'day_of_week_cos' ]\n",
    "\n",
    "        return df5[ cols_selected ]\n",
    "\n",
    "    def get_prediction(self, model, original_data, test_data):\n",
    "        # prediction\n",
    "        pred = model.predict( test_data )\n",
    "\n",
    "        # join pred into the original data\n",
    "        original_data['prediction'] = np.expm1( pred )\n",
    "\n",
    "        return original_data.to_json( orient='records', date_format='iso' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2. Test of Forecasting API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test dataset\n",
    "#import pandas as pd\n",
    "#import json\n",
    "#filepath = \"c:/MeusEstudos/CURSOS TI/Em 2023 - ComunidadeDS/F2.D2 - DS.Prod/Projeto/data/test.csv\"\n",
    "#df10 = pd.read_csv( filepath )\n",
    "\n",
    "MyCSV = 'http://menezes.mendonca.nom.br/datasets/rossmann/test.csv'\n",
    "df10 = pd.read_csv( MyCSV, low_memory=False )\n",
    "\n",
    "#MyCSV = 'http://menezes.mendonca.nom.br/datasets/rossmann/store.csv'\n",
    "#df_store_raw = pd.read_csv( MyCSV, low_memory=False )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge test dataset + store\n",
    "df_test = pd.merge( df10, df_store_raw, how='left', on='Store' )\n",
    "\n",
    "# choose one store for prediction\n",
    "df_test = df_test[ df_test['Store'].isin( [14] ) ]\n",
    "\n",
    "# remove closed days, nulls, remove 'ID'\n",
    "df_test = df_test[ df_test['Open'] != 0 ]\n",
    "df_test = df_test[ ~df_test['Open'].isnull() ]\n",
    "df_test = df_test.drop( 'Id', axis=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Dataframe to Json\n",
    "data = json.dumps( df_test.to_dict( orient='records' ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_formatted_str = json.dumps( df_test.to_dict( orient='records' ), indent=2 )\n",
    "print( json_formatted_str )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before calling the API (next step), go to terminal, change path and run the handler...\n",
    "\n",
    "# cd C:\\MeusEstudos\\CURSOS TI\\Em 2023 - ComunidadeDS\\Projetos do Aluno\\PA.04 Rossmann temp1\\webapp\n",
    "\n",
    "# > python handler.py\n",
    "\n",
    "# stopping...\n",
    "xxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, Response\n",
    "\n",
    "# API Call\n",
    "url = 'http://127.0.0.1:5000/rossmann/predict'\n",
    "#url = 'https://teste-rossmann-priv2.onrender.com/rossmann/predict'\n",
    "header = { 'Content-type': 'application/json' }\n",
    "data = data\n",
    "\n",
    "r = requests.post( url, data=data, headers=header )\n",
    "print( 'Status Code {}'.format( r.status_code ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert API results to a Dataframe\n",
    "d1 = pd.DataFrame( r.json(), columns=r.json()[0].keys() )\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the forecast revenue after six weeks\n",
    "d2 = d1[['store', 'prediction']].groupby( 'store' ).sum().reset_index()\n",
    "\n",
    "for i in range( len( d2 ) ):\n",
    "    print( 'Store Number {} will sell R${:,.2f} in the next 6 weeks'.format(\n",
    "        d2.loc[i, 'store'],\n",
    "        d2.loc[i, 'prediction']\n",
    "    ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3. API Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STOP RUNNING HERE (Next code is for web-api)\n",
    "xxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from flask             import Flask, request, Response\n",
    "from rossmann.Rossmann import Rossmann\n",
    "\n",
    "# loading model\n",
    "model = pickle.load( open( 'c:/MeusEstudos/CURSOS TI/Em 2023 - ComunidadeDS/F2.D2 - DS.Prod/Projeto/parameter/model_rossmann.pkl', 'rb' ) )\n",
    "\n",
    "# init API\n",
    "app = Flask( __name__ )\n",
    "\n",
    "@app.route( '/rossmann/predict', methods=['POST'] )\n",
    "def rossmann_predict():\n",
    "    test_json = request.get_json()\n",
    "\n",
    "    if test_json: # there is data\n",
    "        if isinstance( test_json, dict ):\n",
    "            test_raw = pd.DataFrame( test_json, index=[0] )\n",
    "        else:\n",
    "            test_raw = pd.DataFrame( test_json, columns=test_json[0].keys() )\n",
    "\n",
    "        # Instantiate Rossmann class\n",
    "        pipeline = Rossmann()\n",
    "\n",
    "        # data cleaning\n",
    "        df1 = pipeline.data_cleaning( test_raw )\n",
    "        # feature engineering\n",
    "        df2 = pipeline.feature_engineering( df1 )\n",
    "        # data preparation\n",
    "        df3 = pipeline.data_preparation( df2 )\n",
    "        # prediction\n",
    "        df_response = pipeline.get_prediction( model, test_raw, df3 )\n",
    "\n",
    "        return df_response\n",
    "\n",
    "    else:\n",
    "        return Response( '{}', status=200, mimetype='application/json' )\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run( '0.0.0.0' )    # running at \"localhost\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsprod",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
